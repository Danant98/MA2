{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Assignment 2\n",
    "### MAT-2201 Numerical Methods\n",
    "\n",
    "Daniel Elisabethsønn Antonsen, UiT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries and modules to be used in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "plt.style.use(\"seaborn-whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Let $\\epsilon$ be a positive real number and consider the matrix \n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4 + \\epsilon\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### (a)\n",
    "\n",
    "Here we are asked to apply the $PA = LU$ factorization (with partial pivoting) to matrix A.\n",
    "\n",
    "The $PA = LU$ factorization is just $LU$ factorization with partial pivoting, that is the factorization using the row-exchanged version of matrix A.  \n",
    "And so we want to find the row with the largest value in the first column and move this row to the top of matrix A. So since the matrix is given as\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4 + \\epsilon\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "we can see that the second row has the largest value in the first column, and so we exchange the rows using a permutation matrix $P$ to keep track of the cumulative permutations. And so now we get the matrix\n",
    "$$\n",
    "PA = \\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\\ ,\\ P = \\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We canv now factorization such that $A$ is written as a product of a lower triangular matrix $L$ and a upper triangular matrix $U$. We can use the permutation matrix to keep control of all cumulative permutations of rows, so \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "\\rightarrow \\frac12 \\cdot row 1\\ , row 2 - row 1 \\rightarrow \n",
    "\\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "\\frac12 & -\\frac\\epsilon2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And so the matrix $L$ is given by using $1$'s on the diagonal and the permutations on the lower elements, so we get\n",
    "$$\n",
    "L = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "\\frac12 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The matrix $U$ is the matrix found by Gaussian elimination, and so we get that the matrix is given by\n",
    "$$\n",
    "U = \\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "0 & -\\frac\\epsilon2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And so the $PA = LU$ factorization is given by\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4 + \\epsilon\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "\\frac12 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "0 & -\\frac\\epsilon2\n",
    "\\end{bmatrix}\n",
    "}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "\n",
    "We are here asked to compute the condition number of matrix $A$, $cond(A)$, two times; both by using the infinity-norm and using the $2$-norm. What is the condition number when $\\epsilon = 0.01$?\n",
    "\n",
    "We start by computing the condition number using the infinity-norm $cond_{\\infty} (A)$. Using this, then he condition number is defined as\n",
    "$$\n",
    "cond_{\\infty} (A) = ||A||_{\\infty} ||A^{-1}||_{\\infty}\n",
    "$$\n",
    "where the infinity-norm, $||A||_{\\infty}$, is defined, for an $n\\times n$-matrix as the maximum sum of absolute row values. That is, it is defined by\n",
    "$$\n",
    "||A||_{\\infty} = max(t_{ij}),\\ t_{ij} = \\sum_{j=1}^{n}|a_{ij}|\\ ,\\ for\\ i=1, 2, 3, ..., n\n",
    "$$\n",
    "And so, the sum of absolute values of each row in $A$ is \n",
    "\\begin{align*}\n",
    "|1| + |2| &= 3 \\\\\n",
    "|2| + |4+\\epsilon| &= 6 + \\epsilon\n",
    "\\end{align*}\n",
    "And so, no matter the value of $\\epsilon$, we get \n",
    "$$\n",
    "||A||_{\\infty} = 6 + \\epsilon\n",
    "$$\n",
    "\n",
    "The inverse, $A^{-1}$ of a $2\\times 2$-matrix is given by\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}\n",
    ",\\ \n",
    "A^{-1} = \\frac{1}{det(A)}\\begin{bmatrix}\n",
    "d & -b \\\\\n",
    "-c & a\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "And so, for our matrix $A$ we get\n",
    "\n",
    "\\begin{align*}\n",
    "det(A) &= (1)(4 + \\epsilon) - (2)(2)= 4 + \\epsilon - 4 = \\underline{\\epsilon} \\\\\n",
    "\n",
    "\\Rightarrow A^{-1} &= \\frac1\\epsilon \n",
    "\\begin{bmatrix}\n",
    "4 + \\epsilon & -2 \\\\\n",
    "-2 & 1\n",
    "\\end{bmatrix}\n",
    "= \\underline{\n",
    "\\begin{bmatrix}\n",
    "\\frac{4 + \\epsilon}{\\epsilon} & -\\frac2\\epsilon \\\\\n",
    "-\\frac2\\epsilon & \\frac1\\epsilon\n",
    "\\end{bmatrix}\n",
    "}\n",
    "\\end{align*}\n",
    "\n",
    "So the sum of absolute values for each row in matrix $A^{-1}$ is then\n",
    "\n",
    "\\begin{align*}\n",
    "\\bigg| \\frac{4 + \\epsilon}{\\epsilon} \\bigg| + \\bigg| -\\frac2\\epsilon \\bigg| &= \\underline{\\frac{6 + \\epsilon}{\\epsilon}} \\\\\n",
    "\\bigg| -\\frac2\\epsilon \\bigg| + \\bigg| \\frac1\\epsilon \\bigg| &= \\underline{\\frac{3}{\\epsilon}}\n",
    "\\end{align*}\n",
    "\n",
    "And so no matter the value of $\\epsilon$, we get\n",
    "$$\n",
    "||A^{-1}||_{\\infty} = \\frac{6 + \\epsilon}{\\epsilon}\n",
    "$$\n",
    "\n",
    "So the condition number, $cond_{\\infty}$ is given as\n",
    "\n",
    "\\begin{align*}\n",
    "cond_{\\infty} (A) &= ||A||_{\\infty} ||A^{-1}||_{\\infty} \\\\\n",
    "                  &= (6 + \\epsilon) \\left(\\frac{6 + \\epsilon}{\\epsilon} \\right) \\\\\n",
    "                  &= \\boxed{\\frac{(6 + \\epsilon)^2}{\\epsilon}}\n",
    "\\end{align*}\n",
    "\n",
    "If we now have that $\\epsilon = 0.01$, we get that the condition number has the value of\n",
    "$$\n",
    "\\boxed{cond_{\\infty} (A) = 3612.01}\n",
    "$$\n",
    "\n",
    "Now that we have used the infinity-norm to compute the condition number of matrix $A$, then we can move on to computing using the $2$-norm, i.e. $||A||_{2}$. And so the condition number $cond_{2} (A)$ is defined using $2$-norm as\n",
    "$$\n",
    "cond_{2} (A) = ||A||_{2} ||A^{-1}||_{2}\n",
    "$$\n",
    "where $||A||_2 = \\sqrt{\\rho(A^{T}A)} = \\sqrt{\\rho(B)}$, $B = A^{T}A$. \n",
    "\n",
    "That is, the $2$-norm of matrix $A$ is the square root of the spectral radius of matrix $B$, which is the matrixmultiplication of $A$ and $A^{T}$. Spectral radius is simply speaking the maximal absolute value of the eigenvalues of a matrix.  \n",
    "And so, we can find the $2$-norm of matrix $A$ by first computing the matrix $B$. Then finding it's eigenvalues and pick the largest absolute value of the eigenvalues. And so $||A||_2$ is the square root of this value.\n",
    "\n",
    "So we start by computing $B$\n",
    "$$\n",
    "B = A^{T} A\n",
    "$$\n",
    "where $A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 + \\epsilon \\end{bmatrix}$\n",
    "\n",
    "And so \n",
    "$$\n",
    "B = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 + \\epsilon \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 + \\epsilon \\end{bmatrix}\n",
    "= \\begin{bmatrix} 5 & 2\\epsilon + 10 \\\\ 2\\epsilon + 10 & \\epsilon^2 + 8\\epsilon + 20 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If $A$ is an $n\\times n$-matrix and $x$ a non-zero $n$-dimensional real or complex vector. If we also have \n",
    "$$\n",
    "Ax = \\lambda x\n",
    "$$\n",
    "where $\\lambda$ is some real of complex number. Then $\\lambda$ is an eigenvalue and $x$ is the corresponding eigenvector.  \n",
    "We can find the eigenvalues by computing the roots of the characteristic polynomial \n",
    "$$\n",
    "det(\\lambda I - A) = 0\n",
    "$$\n",
    "$I$ is the identity matrix, $I = \\begin{bmatrix} 1 & & 0 \\\\ &\\ddots& \\\\ 0 & & 1 \\end{bmatrix}$. \n",
    "\n",
    "And so we can compute\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "det(\\lambda I - A) &= \\begin{vmatrix} \\lambda - 5 & -2\\epsilon - 10 \\\\ -2\\epsilon - 10 & \\lambda - \\epsilon^2 - 8\\epsilon - 20 \\end{vmatrix} \\\\\n",
    "\n",
    "&= (\\lambda - 5)(\\lambda - \\epsilon^2 - 8\\epsilon - 20) - (-2\\epsilon - 10)(-2\\epsilon - 10) \\\\\n",
    "&= \\lambda^2 - \\lambda\\epsilon^2 - 8\\epsilon\\lambda - 25\\lambda + \\epsilon^2 \\\\\n",
    "&= \\lambda^2 - \\lambda(\\epsilon^2 + 8\\epsilon + 25) + \\epsilon^2 = 0\n",
    "\\end{align*}\n",
    "\n",
    "So further we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_{\\pm} = \\underline{\\frac{\\epsilon^2 + 8\\epsilon + 25 \\pm \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}{2}}\n",
    "\\end{align*}\n",
    "\n",
    "Since $\\epsilon > 0$, then the root will always be larger than zero $\\forall \\epsilon$. And so \n",
    "$$\n",
    "\\rho(B) = \\underline{\\frac{\\epsilon^2 + 8\\epsilon + 25 + \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}{2}}\n",
    "$$\n",
    "And so the $2$-norm of matrix $A$ is given as\n",
    "$$\n",
    "||A||_2 = \\sqrt{\\rho(B)} = \\sqrt{\\frac{\\epsilon^2 + 8\\epsilon + 25 + \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}{2}}\n",
    "$$\n",
    "\n",
    "Further we need to compute the $2$-norm of $A^{-1}$. To do this, we can use some simple matrix rules. And so we have \n",
    "\n",
    "\\begin{align*}\n",
    "(A^T A)x = \\lambda x \\Rightarrow (A^T A)^{-1} (A^T A) x = (A^T A)^{-1} \\lambda x \\\\\n",
    "\\Rightarrow \\frac1\\lambda x = \\lambda^{-1} x = (A^T A)^{-1} x\n",
    "\\end{align*}\n",
    "And so, we only need to compute the inverse of the eigenvalues to find the eigenvalues for $A^{-1}$. This means that the eigenvaleus is given as\n",
    "$$\n",
    "\\lambda_{\\pm}^{-1} = \\frac{2}{\\epsilon^2 + 8\\epsilon + 25 \\pm \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}\n",
    "$$\n",
    "\n",
    "Further we have that the max value will be the eigenvalue with the lowest denominator, and so\n",
    "$$\n",
    "\\rho((A^T A)^{-1}) = \\frac{2}{\\epsilon^2 + 8\\epsilon + 25 - \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}\n",
    "$$\n",
    "\n",
    "And so the $2$-norm of matrix $A^{-1}$ is given as\n",
    "$$\n",
    "||A^{-1}||_2 = \\sqrt{\\frac{2}{\\epsilon^2 + 8\\epsilon + 25 - \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}}\n",
    "$$\n",
    "\n",
    "We can compute now compute the condition number $cond_2 (A)$ by\n",
    "\n",
    "\\begin{align*}\n",
    "cond_2 (A) &= ||A||_2 ||A^{-1}||_2 \\\\\n",
    "&= \\boxed{\\sqrt{\\frac{\\epsilon^2 + 8\\epsilon + 25 + \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}{\\epsilon^2 + 8\\epsilon + 25 - \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}}}\n",
    "\\end{align*}\n",
    "\n",
    "Now we can use $\\epsilon = 0.01$, which gives\n",
    "$$\n",
    "\\boxed{cond_2 (A) = 2508.01}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)\n",
    "\n",
    "Here we are asked to use the result from (a) to solve the system $Ax = b$ where\n",
    "$$\n",
    "b = \\begin{bmatrix}\n",
    "2.9 \\\\\n",
    "6.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Then what is the solution when $\\epsilon = 0.01$? \n",
    "\n",
    "For solving the system \n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "we can start by multiplying the system with the matrix $P$, and so\n",
    "$$\n",
    "PAx = Pb \n",
    "= \\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2.9 \\\\\n",
    "6.2\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "6.2 \\\\\n",
    "2.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "of if we factorize the system using the factorization found in (a), and so we get the system on the form\n",
    "$$\n",
    "LUx = Pb\n",
    "$$\n",
    "We can now solve the system\n",
    "$$\n",
    "1.\\ Lc = Pb\\ ,\\ for\\ c = \\begin{bmatrix} c_1 & c_2 \\end{bmatrix}^{T} \\\\\n",
    "2.\\ Ux = c\\ ,\\ for\\ x = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix}^{T}\n",
    "$$\n",
    "\n",
    "We start by computing $1.$ where $L$ is the lower triangular matrix found in (a), and so \n",
    "\n",
    "\\begin{align*}\n",
    "Lc &= Pb \\\\\n",
    "\\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "\\frac12 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{bmatrix}\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "6.2 \\\\\n",
    "2.9\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "And so \n",
    "\n",
    "\\begin{align*}\n",
    "\\Rightarrow \\underline{c_1 = 6.2} \\\\\n",
    "\\frac12 c_1 + c_2 &= 2.9 \\Rightarrow \\underline{c_2 = 2.9 - 3.1 = -0.2}\n",
    "\\end{align*}\n",
    "\n",
    "Futher we can use\n",
    "$$\n",
    "c = \\begin{bmatrix} 6.2 \\\\ -0.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "to compute $2.$ where $U$ is the upper triangular matrix found in (a), and so\n",
    "\n",
    "\\begin{align*}\n",
    "Ux &= c \\\\\n",
    "\\Rightarrow \n",
    "\\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "0 & -\\frac\\epsilon2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "x_1 \\\\ \n",
    "x_2 \n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix} \n",
    "6.2 \\\\ \n",
    "-0.2 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "And so we can start from the bottom and compute $x_2$ then compute $x_1$\n",
    "\n",
    "\\begin{align*}\n",
    "-\\frac\\epsilon2 x_2 = -0.2 \\Rightarrow \\underline{x_2 = \\frac{0.4}{\\epsilon}} \\\\\n",
    "\n",
    "2x_1 + (4 + \\epsilon)x_2 &= 6.2 \\\\\n",
    "2x_1 &= 6.2 - (4 + \\epsilon)\\left(\\frac{0.4}{\\epsilon} \\right) \\\\\n",
    "&= 6.2 - \\frac{1.6}{\\epsilon} - 0.4 \\\\\n",
    "&= 5.8 - \\frac{1.6}{\\epsilon} \\\\\n",
    "\n",
    "\\Rightarrow \\underline{x_1 = 2.9 - \\frac{0.8}{\\epsilon}}\n",
    "\\end{align*}\n",
    "\n",
    "So the solution to this system is\n",
    "$$\n",
    "\\boxed{x = \\begin{bmatrix} 2.9 - \\frac{0.8}{\\epsilon} \\\\ \\frac{0.4}{\\epsilon} \\end{bmatrix}}\n",
    "$$\n",
    "\n",
    "If now we have $\\epsilon = 0.01$, then we get the solution\n",
    "$$\n",
    "\\boxed{x = \\begin{bmatrix} -77.1 \\\\ 40 \\end{bmatrix}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)\n",
    "\n",
    "If now the values for $b$, $2.9$ and $6.2$ is some measured values. And it turns out that a more precise measurement is given as\n",
    "$$\n",
    "b = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\n",
    "$$\n",
    "Repeat (c) for new $b$. What is now the solution for $\\epsilon = 0.01$? \n",
    "\n",
    "Here again we can solve the system \n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "by using the same procedure as in (c). And so we start på multiplying the matrix $P$ into the system, and so we get\n",
    "$$\n",
    "PAx = Pb = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "And here again we can write $PA$ as the factorization $LU$ where $L$ is the lower triangular matrix and $U$ is the upper triangular matrix. So we need to solve the system\n",
    "$$\n",
    "1.\\ Lc = Pb \\\\\n",
    "2.\\ Ux = c\n",
    "$$\n",
    "\n",
    "We start by computing the first equation, and so\n",
    "\n",
    "\\begin{align*}\n",
    "Lc &= Pb \\\\\n",
    "\\Rightarrow \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "\\frac12 & 1 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{bmatrix}\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "6 \\\\ \n",
    "3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "which gives \n",
    "\\begin{align*}\n",
    "c_1 &= \\underline{6} \\\\\n",
    "\\frac12 c_1 + c_2 &= 3 \\Rightarrow c_2 = 3 - \\frac12 (6) = \\underline{0}\n",
    "\\end{align*}\n",
    "\n",
    "so the vector $c$ is given by \n",
    "$$\n",
    "c = \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can now continue by computing \n",
    "\n",
    "\\begin{align*}\n",
    "Ux &= c \\\\\n",
    "\\Rightarrow \n",
    "\\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "0 & -\\frac\\epsilon2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ x_2\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "6 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "which gives \n",
    "\\begin{align*}\n",
    "-\\frac\\epsilon2 x_2 = 0 \\Rightarrow \\underline{x_2 = 0}\n",
    "2x_1 + (4 + \\epsilon) x_2 &= 6 \\\\\n",
    "2x_1 + (4 + \\epsilon) (0) &= 6 \\\\\n",
    "2x_1 &= 6 \\Rightarrow \\underline{x_2 = 3} \n",
    "\\end{align*}\n",
    "\n",
    "So the solution to the system is \n",
    "$$\n",
    "\\boxed{x = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}}\n",
    "$$\n",
    "\n",
    "When $\\epsilon = 0.01$, then the solution to the system is unchanged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e)\n",
    "\n",
    "If we denote the solution from (c) \n",
    "$$\n",
    "x_a = \\begin{bmatrix} -77.1 \\\\ 40 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and the solution from (d) is denoted \n",
    "$$\n",
    "x_0 = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$x_a$ is the approximated solution for the system \n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\epsilon = 0.01,\\ A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4.01 \\end{bmatrix},\\ b = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can compute using both infinity-norm and $2$-norm. We start by using the infinity-norm. And so the backward error can be computed by\n",
    "$$\n",
    "||b - Ax_a ||_{\\infty} \\\\\n",
    "$$\n",
    "$$\n",
    "Ax_a = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4.01 \\end{bmatrix} \\begin{bmatrix} -77.1 \\\\ 40 \\end{bmatrix} = \n",
    "\\begin{bmatrix} 2.9 \\\\ 6.2 \\end{bmatrix}\n",
    "$$\n",
    "And so the backwards error using the infinity-norm is given as\n",
    "$$\n",
    "||b - Ax_a||_\\infty = ||\\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 2.9 \\\\ 6.2 \\end{bmatrix}||_\\infty\n",
    "= ||\\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}||_{\\infty} = \\boxed{0.2}\n",
    "$$\n",
    "\n",
    "The forward error using the infinity-norm is given by \n",
    "$$\n",
    "||x_0 - x_a||_\\infty \n",
    "$$\n",
    "And so\n",
    "$$\n",
    "||\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} -77.1 \\\\ 40 \\end{bmatrix}||_\\infty \n",
    "= ||\\begin{bmatrix} 80.1 \\\\ -40 \\end{bmatrix}||_\\infty = \\boxed{80.1}\n",
    "$$\n",
    "\n",
    "To comnpute the corresponding error magnification factor, we must first compute the relative backward error and the relative forward error using the infinity-norm. The backward error is defined as \n",
    "$$\n",
    "\\frac{||b - Ax_a||_\\infty}{||b||_\\infty}\n",
    "$$\n",
    "\n",
    "And so we get \n",
    "$$\n",
    "\\frac{||b - Ax_a||_\\infty}{||b||_\\infty} = \\frac{0.2}{||\\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}||_\\infty} = \\frac{0.2}{6} \\approx \\underline{0.0333}  \n",
    "$$\n",
    "\n",
    "And the relative forward error is given by\n",
    "$$\n",
    "\\frac{||x_0 - x_a||_\\infty}{||x_0||_\\infty}\n",
    "$$\n",
    "\n",
    "And so we get that the relative forward error is \n",
    "$$\n",
    "\\frac{||x_0 - x_a||_\\infty}{||x_0||_\\infty} = \\frac{80.1}{||\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}||_\\infty} = \n",
    "\\frac{80.1}{3} = \\underline{26.7}\n",
    "$$\n",
    "\n",
    "We can now compute the error magnification factor for the system $Ax = b$, which is defined using infinity-norm as \n",
    "$$\n",
    "error\\ mf = \\frac{relative\\ forward\\ error}{relative\\ backward\\ error} = \\frac{\\frac{||b - Ax_a||_\\infty}{||b||_\\infty}}{\\frac{||x_0 - x_a||_\\infty}{||x_0||_\\infty}}\n",
    "$$\n",
    "\n",
    "And so the error magnification factor is \n",
    "$$\n",
    "\\frac{26.7}{0.0333} = \\boxed{801.802}\n",
    "$$\n",
    "\n",
    "In (b) we found that the condition number using the infinity-norm is $3612.01$, and the condtion number tells us the maximal error magnification factor. And here we get a value that is well below the maximum, but in fact still high. And so a small change in value, will mean a large difference in answers.\n",
    "\n",
    "Now we will do the same computations using the $2$-norm. The backward error is defined as \n",
    "$$\n",
    "||b - Ax_a||_2\n",
    "$$\n",
    "$$\n",
    "b - Ax_a = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 2.9 \\\\ 6.2 \\end{bmatrix} = \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The $2$-norm of a given vector $x = \\begin{bmatrix} x_1 & x_2 & \\dots & \\end{bmatrix}^T$ is defined as \n",
    "$$\n",
    "||x||_2 = \\sqrt{x_1^2 + x_2^2 + \\dots}\n",
    "$$\n",
    "This is also called the Euclidean norm. And so the backward error is \n",
    "$$\n",
    "||b - Ax_a||_2 = ||\\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}||_2 = \\sqrt{(0.1)^2 + (-0.2)^2} = \\boxed{0.2236}\n",
    "$$\n",
    "\n",
    "Further we can compute the forward error, which is defined as \n",
    "$$\n",
    "||x_0 - x_a||_2\n",
    "$$\n",
    "and so the forward error is then \n",
    "$$\n",
    "\\Rightarrow ||\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} -77.1 \\\\ 40 \\end{bmatrix}||_2\n",
    "= ||\\begin{bmatrix} 80.1 \\\\ -40 \\end{bmatrix}||_2 = \\sqrt{(80.1)^2 + (-40)^2} = \\boxed{89.523}\n",
    "$$\n",
    "\n",
    "Here again we need to find the  relative backward error and the relative forward error.  \n",
    "And so \n",
    "$$\n",
    "\\frac{||b - Ax_a||_2}{||b||_2}\n",
    "$$\n",
    "$$\n",
    "||b||_2 = ||\\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}||_2 = \\sqrt{3^2 + 6^2} = \\underline{6.708}\n",
    "$$\n",
    "\n",
    "And the relative forward error is given as \n",
    "$$\n",
    "\\frac{||x_0 - x_a||_2}{||x_0||_2}\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow ||x_0||_2 = ||\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}||_2 = \\underline{3}\n",
    "$$\n",
    "And so the error magnification factor is given as \n",
    "$$\n",
    "error\\ mf = \\frac{\\frac{||x_0 - x_a||_2}{||x_0||_2}}{\\frac{||b - Ax_a||_2}{||b||_2}} = \\frac{\\frac{89.523}{3}}{\\frac{0.2236}{6.708}} = \\boxed{895.32}\n",
    "$$\n",
    "\n",
    "Here again the value is well below the condition number, which checks out since the condition number is the maximal error magnification factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f)\n",
    "\n",
    "Using the $LU$-factorization we can transform a system into easier and simpler ways of calculating if we have different $b$ vectors. And some systems of the form $Ax = b$ is very sensitive to initial changes in the output $b$. A small change in $b$ can lead to very different results for the vector $x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Consider a set of data points $\\{(x_1, y_1),... ,(x_N, y_N)\\}$. Suppose that the relationship between the $x$'s and $y$'s can be expressed as $y_i = g_k (x_i)$, where $k$ is a parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Here we are given the following function, which is a modification of the Mean Squared Error:\n",
    "$$\n",
    "F(k) = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - g_k (x_i))^4\n",
    "$$\n",
    "What does this function represent? What is the meaning of its minimum? \n",
    "\n",
    "Since this is a modification of the Mean Squared Error, then it is just the double squared mean error. And so it describes the relationship between between the $x$'s and $y$'s. The minimum describes the the minimum error between the $x$'s and $y$'s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "\n",
    "From now on,m we will assume that $g_k (x) = kx$. Using this, we are asked to compute the derivative of $F(k)$ with respect to $k$. \n",
    "$$\n",
    "f(k) = \\frac{\\partial F}{\\partial k}(k)\n",
    "$$\n",
    "\n",
    "And so we can compute\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial F}{\\partial k}(k) &= \\frac{\\partial}{\\partial k}\\left(\\frac{1}{N}\\sum_{i=1}^{N}(y_i - kx_i)^4 \\right) \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\partial}{\\partial k} (y_i - kx_i)^4 \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^{N} 4(y_i - kx_i)^3 (-x_i) \\\\\n",
    "&= -\\frac{4}{N}\\sum_{i=1}^{N} x_i(y_i - kx_i)^3\n",
    "\\end{align*}\n",
    "And so the derivative of $F(k)$ is given as\n",
    "$$\n",
    "\\boxed{f(k) = \\frac{\\partial F}{\\partial k}(k) = -\\frac{4}{N}\\sum_{i=1}^{N} x_i(y_i - kx_i)^3}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)\n",
    "\n",
    "Let us now condsider $N = 5$ and the data points given below with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining each data point given in the problem\n",
    "x1, y1 = -0.14636714, -0.01101692\n",
    "x2, y2 = 1.03206087, 0.46129383\n",
    "x3, y3 = 1.87784546, 0.96929284\n",
    "x4, y4 = 2.9766235, 1.40264061\n",
    "x5, y5 = 3.79677605, 2.0512337\n",
    "# Creating array to hold all data points \n",
    "data_points = np.array([[x1, y1], [x2, y2], [x3, y3], [x4, y4], [x5, y5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asked to use Newton's method to find an approximation of the minimum $\\bar{k}$ of the function $F(k)$, that is when the derivative $f(k) = 0$. We are asked to have a table with all intermediate results along with the errors $e_i = |k_i - r|$ and both ratios $e_i/e_{i-1}$ and $e_i/e_{i-1}^2$.\n",
    "\n",
    "The Newton's method or also called Newton-Raphson method is a method that usually converges faster than linearly convergent methods. This method finds the root of function on the form $f(x) = 0$, where we start by guessing a initial value for the root and use the tangent line at the point $(x_0, f(x_0))$ where $x_0$ is our initial guessed value. The $x$-value where the tangeng line intersects the $x$-axis, is our new value for the approximated root. Doing this over again will yield a better and more accurate approximation, and so we iterate over several values for $x$.  \n",
    "\n",
    "We can find a formula for Newton-Raphson method by considering the formula for point-slope for a point $(x_0, f(x_0))$, given as\n",
    "$$\n",
    "y - f(x_0) = f'(x_0)(x - x_0)\n",
    "$$\n",
    "Setting $y = 0$ and solve for $x$ gives us the approximation for the root\n",
    "$$\n",
    "x = x_0 - \\frac{f(x_0)}{f'(x_0)}\n",
    "$$\n",
    "\n",
    "And so the iterative Newton-Raphson method has the formula \n",
    "$$\n",
    "x_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)},\\ for\\ x = 0, 1, 2, ...\n",
    "$$\n",
    "\n",
    "As we can see from the fomula for the Newton-Raphson method, we need both the function $f(k)$ and its derivative. The function is found in (b), but we need to compute the derivative of $f(k)$ before we can implement it using python. And so we have to\n",
    "$$\n",
    "f'(k) = \\frac{\\partial^2}{\\partial k^2}F(k)\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\Rightarrow f'(k) &= \\frac{\\partial}{\\partial k} \\left(-\\frac{4}{N}\\sum_{i=1}^{N} x_i(y_i - kx_i)^3\\right) \\\\\n",
    "&= -\\frac{4}{N}\\sum_{i=1}^{N} \\frac{\\partial}{\\partial k}\\left(x_i (y_i - kx_i)^3\\right) \\\\\n",
    "&= -\\frac{12}{N}\\sum_{i=1}^{N} x_i(y_i - kx_i)^2 (-x_i) \\\\\n",
    "&= \\underline{\\frac{12}{N}\\sum_{i=1}^{N} x_i^2(y_i - kx_i)^2}\n",
    "\\end{align*}\n",
    "\n",
    "Now that we have both the function $f(k)$ and it's derivative $f'(k)$, we can implement these. This is done below using the data points and $N = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining arrays for x and y values\n",
    "x = data_points[:, 0]\n",
    "y = data_points[:, 1]\n",
    "# Defining function f and derivative of f\n",
    "f = lambda k: -(4 / 5) * np.sum(x * (y - k * x)**3)\n",
    "df = lambda k: (12 / 5) * np.sum(x**2 * (y - k * x)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement a function to compute the iterative Newton-Raphson method, the error and both the ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton_Rapson(f:function, df:function, tol:float, itera:int):\n",
    "    \"\"\"\n",
    "    Function for compute the Newton-Raphson method \n",
    "\n",
    "    Input:\n",
    "        f: function, function we want to find the root for\n",
    "        df: function, derivate of the function f\n",
    "        tol: \n",
    "    \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
