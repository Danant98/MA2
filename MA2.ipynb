{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Assignment 2\n",
    "### MAT-2201 Numerical Methods\n",
    "\n",
    "Daniel ElisabethsÃ¸nn Antonsen, UiT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries and modules to be used in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "plt.style.use(\"seaborn-whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Let $\\epsilon$ be a positive real number and consider the matrix \n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4 + \\epsilon\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### (a)\n",
    "\n",
    "Here we are asked to apply the $PA = LU$ factorization (with partial pivoting) to matrix A.\n",
    "\n",
    "The $PA = LU$ factorization is just $LU$ factorization with partial pivoting, that is the factorization using the row-exchanged version of matrix A.  \n",
    "And so we want to find the row with the largest value in the first column and move this row to the top of matrix A. So since the matrix is given as\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4 + \\epsilon\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "we can see that the second row has the largest value in the first column, and so we exchange the rows using a permutation matrix $P$ to keep track of the cumulative permutations. And so now we get the matrix\n",
    "$$\n",
    "PA = \\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\\ ,\\ P = \\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We canv now factorization such that $A$ is written as a product of a lower triangular matrix $L$ and a upper triangular matrix $U$. We can use the permutation matrix to keep control of all cumulative permutations of rows, so \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "\\rightarrow \\frac12 \\cdot row 1\\ , row 2 - row 1 \\rightarrow \n",
    "\\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "\\frac12 & -\\frac\\epsilon2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And so the matrix $L$ is given by using $1$'s on the diagonal and the permutations on the lower elements, so we get\n",
    "$$\n",
    "L = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "\\frac12 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The matrix $U$ is the matrix found by Gaussian elimination, and so we get that the matrix is given by\n",
    "$$\n",
    "U = \\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "0 & -\\frac\\epsilon2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And so the $PA = LU$ factorization is given by\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4 + \\epsilon\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "\\frac12 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "0 & -\\frac\\epsilon2\n",
    "\\end{bmatrix}\n",
    "}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "\n",
    "We are here asked to compute the condition number of matrix $A$, $cond(A)$, two times; both by using the infinity-norm and using the $2$-norm. What is the condition number when $\\epsilon = 0.01$?\n",
    "\n",
    "We start by computing the condition number using the infinity-norm $cond_{\\infty} (A)$. Using this, then he condition number is defined as\n",
    "$$\n",
    "cond_{\\infty} (A) = ||A||_{\\infty} ||A^{-1}||_{\\infty}\n",
    "$$\n",
    "where the infinity-norm, $||A||_{\\infty}$, is defined, for an $n\\times n$-matrix as the maximum sum of absolute row values. That is, it is defined by\n",
    "$$\n",
    "||A||_{\\infty} = max(t_{ij}),\\ t_{ij} = \\sum_{j=1}^{n}|a_{ij}|\\ ,\\ for\\ i=1, 2, 3, ..., n\n",
    "$$\n",
    "And so, the sum of absolute values of each row in $A$ is \n",
    "\\begin{align*}\n",
    "|1| + |2| &= 3 \\\\\n",
    "|2| + |4+\\epsilon| &= 6 + \\epsilon\n",
    "\\end{align*}\n",
    "And so, no matter the value of $\\epsilon$, we get \n",
    "$$\n",
    "||A||_{\\infty} = 6 + \\epsilon\n",
    "$$\n",
    "\n",
    "The inverse, $A^{-1}$ of a $2\\times 2$-matrix is given by\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}\n",
    ",\\ \n",
    "A^{-1} = \\frac{1}{det(A)}\\begin{bmatrix}\n",
    "d & -b \\\\\n",
    "-c & a\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "And so, for our matrix $A$ we get\n",
    "\n",
    "\\begin{align*}\n",
    "det(A) &= (1)(4 + \\epsilon) - (2)(2)= 4 + \\epsilon - 4 = \\underline{\\epsilon} \\\\\n",
    "\n",
    "\\Rightarrow A^{-1} &= \\frac1\\epsilon \n",
    "\\begin{bmatrix}\n",
    "4 + \\epsilon & -2 \\\\\n",
    "-2 & 1\n",
    "\\end{bmatrix}\n",
    "= \\underline{\n",
    "\\begin{bmatrix}\n",
    "\\frac{4 + \\epsilon}{\\epsilon} & -\\frac2\\epsilon \\\\\n",
    "-\\frac2\\epsilon & \\frac1\\epsilon\n",
    "\\end{bmatrix}\n",
    "}\n",
    "\\end{align*}\n",
    "\n",
    "So the sum of absolute values for each row in matrix $A^{-1}$ is then\n",
    "\n",
    "\\begin{align*}\n",
    "\\bigg| \\frac{4 + \\epsilon}{\\epsilon} \\bigg| + \\bigg| -\\frac2\\epsilon \\bigg| &= \\underline{\\frac{6 + \\epsilon}{\\epsilon}} \\\\\n",
    "\\bigg| -\\frac2\\epsilon \\bigg| + \\bigg| \\frac1\\epsilon \\bigg| &= \\underline{\\frac{3}{\\epsilon}}\n",
    "\\end{align*}\n",
    "\n",
    "And so no matter the value of $\\epsilon$, we get\n",
    "$$\n",
    "||A^{-1}||_{\\infty} = \\frac{6 + \\epsilon}{\\epsilon}\n",
    "$$\n",
    "\n",
    "So the condition number, $cond_{\\infty}$ is given as\n",
    "\n",
    "\\begin{align*}\n",
    "cond_{\\infty} (A) &= ||A||_{\\infty} ||A^{-1}||_{\\infty} \\\\\n",
    "                  &= (6 + \\epsilon) \\left(\\frac{6 + \\epsilon}{\\epsilon} \\right) \\\\\n",
    "                  &= \\boxed{\\frac{(6 + \\epsilon)^2}{\\epsilon}}\n",
    "\\end{align*}\n",
    "\n",
    "If we now have that $\\epsilon = 0.01$, we get that the condition number has the value of\n",
    "$$\n",
    "\\boxed{cond_{\\infty} (A) = 3612.01}\n",
    "$$\n",
    "\n",
    "Now that we have used the infinity-norm to compute the condition number of matrix $A$, then we can move on to computing using the $2$-norm, i.e. $||A||_{2}$. And so the condition number $cond_{2} (A)$ is defined using $2$-norm as\n",
    "$$\n",
    "cond_{2} (A) = ||A||_{2} ||A^{-1}||_{2}\n",
    "$$\n",
    "where $||A||_2 = \\sqrt{\\rho(A^{T}A)} = \\sqrt{\\rho(B)}$, $B = A^{T}A$. \n",
    "\n",
    "That is, the $2$-norm of matrix $A$ is the square root of the spectral radius of matrix $B$, which is the matrixmultiplication of $A$ and $A^{T}$. Spectral radius is simply speaking the maximal absolute value of the eigenvalues of a matrix.  \n",
    "And so, we can find the $2$-norm of matrix $A$ by first computing the matrix $B$. Then finding it's eigenvalues and pick the largest absolute value of the eigenvalues. And so $||A||_2$ is the square root of this value.\n",
    "\n",
    "So we start by computing $B$\n",
    "$$\n",
    "B = A^{T} A\n",
    "$$\n",
    "where $A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 + \\epsilon \\end{bmatrix}$\n",
    "\n",
    "And so \n",
    "$$\n",
    "B = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 + \\epsilon \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 + \\epsilon \\end{bmatrix}\n",
    "= \\begin{bmatrix} 5 & 2\\epsilon + 10 \\\\ 2\\epsilon + 10 & \\epsilon^2 + 8\\epsilon + 20 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If $A$ is an $n\\times n$-matrix and $x$ a non-zero $n$-dimensional real or complex vector. If we also have \n",
    "$$\n",
    "Ax = \\lambda x\n",
    "$$\n",
    "where $\\lambda$ is some real of complex number. Then $\\lambda$ is an eigenvalue and $x$ is the corresponding eigenvector.  \n",
    "We can find the eigenvalues by computing the roots of the characteristic polynomial \n",
    "$$\n",
    "det(\\lambda I - A) = 0\n",
    "$$\n",
    "$I$ is the identity matrix, $I = \\begin{bmatrix} 1 & & 0 \\\\ &\\ddots& \\\\ 0 & & 1 \\end{bmatrix}$. \n",
    "\n",
    "And so we can compute\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "det(\\lambda I - A) &= \\begin{vmatrix} \\lambda - 5 & -2\\epsilon - 10 \\\\ -2\\epsilon - 10 & \\lambda - \\epsilon^2 - 8\\epsilon - 20 \\end{vmatrix} \\\\\n",
    "\n",
    "&= (\\lambda - 5)(\\lambda - \\epsilon^2 - 8\\epsilon - 20) - (-2\\epsilon - 10)(-2\\epsilon - 10) \\\\\n",
    "&= \\lambda^2 - \\lambda\\epsilon^2 - 8\\epsilon\\lambda - 25\\lambda + \\epsilon^2 \\\\\n",
    "&= \\lambda^2 - \\lambda(\\epsilon^2 + 8\\epsilon + 25) + \\epsilon^2 = 0\n",
    "\\end{align*}\n",
    "\n",
    "So further we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_{\\pm} = \\underline{\\frac{\\epsilon^2 + 8\\epsilon + 25 \\pm \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}{2}}\n",
    "\\end{align*}\n",
    "\n",
    "Since $\\epsilon > 0$, then the root will always be larger than zero $\\forall \\epsilon$. And so \n",
    "$$\n",
    "\\rho(B) = \\underline{\\frac{\\epsilon^2 + 8\\epsilon + 25 + \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}{2}}\n",
    "$$\n",
    "And so the $2$-norm of matrix $A$ is given as\n",
    "$$\n",
    "||A||_2 = \\sqrt{\\rho(B)} = \\sqrt{\\frac{\\epsilon^2 + 8\\epsilon + 25 + \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}{2}}\n",
    "$$\n",
    "\n",
    "Further we need to compute the $2$-norm of $A^{-1}$. To do this, we can use some simple matrix rules. And so we have \n",
    "\n",
    "\\begin{align*}\n",
    "(A^T A)x = \\lambda x \\Rightarrow (A^T A)^{-1} (A^T A) x = (A^T A)^{-1} \\lambda x \\\\\n",
    "\\Rightarrow \\frac1\\lambda x = \\lambda^{-1} x = (A^T A)^{-1} x\n",
    "\\end{align*}\n",
    "And so, we only need to compute the inverse of the eigenvalues to find the eigenvalues for $A^{-1}$. This means that the eigenvaleus is given as\n",
    "$$\n",
    "\\lambda_{\\pm}^{-1} = \\frac{2}{\\epsilon^2 + 8\\epsilon + 25 \\pm \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}\n",
    "$$\n",
    "\n",
    "Further we have that the max value will be the eigenvalue with the lowest denominator, and so\n",
    "$$\n",
    "\\rho((A^T A)^{-1}) = \\frac{2}{\\epsilon^2 + 8\\epsilon + 25 - \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}\n",
    "$$\n",
    "\n",
    "And so the $2$-norm of matrix $A^{-1}$ is given as\n",
    "$$\n",
    "||A^{-1}||_2 = \\sqrt{\\frac{2}{\\epsilon^2 + 8\\epsilon + 25 - \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}}\n",
    "$$\n",
    "\n",
    "We can compute now compute the condition number $cond_2 (A)$ by\n",
    "\n",
    "\\begin{align*}\n",
    "cond_2 (A) &= ||A||_2 ||A^{-1}||_2 \\\\\n",
    "&= \\boxed{\\sqrt{\\frac{\\epsilon^2 + 8\\epsilon + 25 + \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}{\\epsilon^2 + 8\\epsilon + 25 - \\sqrt{(\\epsilon^2 + 8\\epsilon + 25)^2 - 4\\epsilon^2}}}}\n",
    "\\end{align*}\n",
    "\n",
    "Now we can use $\\epsilon = 0.01$, which gives\n",
    "$$\n",
    "\\boxed{cond_2 (A) = 2508.01}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)\n",
    "\n",
    "Here we are asked to use the result from (a) to solve the system $Ax = b$ where\n",
    "$$\n",
    "b = \\begin{bmatrix}\n",
    "2.9 \\\\\n",
    "6.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Then what is the solution when $\\epsilon = 0.01$? \n",
    "\n",
    "For solving the system \n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "we can start by multiplying the system with the matrix $P$, and so\n",
    "$$\n",
    "PAx = Pb \n",
    "= \\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2.9 \\\\\n",
    "6.2\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "6.2 \\\\\n",
    "2.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "of if we factorize the system using the factorization found in (a), and so we get the system on the form\n",
    "$$\n",
    "LUx = Pb\n",
    "$$\n",
    "We can now solve the system\n",
    "$$\n",
    "1.\\ Lc = Pb\\ ,\\ for\\ c = \\begin{bmatrix} c_1 & c_2 \\end{bmatrix}^{T} \\\\\n",
    "2.\\ Ux = c\\ ,\\ for\\ x = \\begin{bmatrix} x_1 & x_2 \\end{bmatrix}^{T}\n",
    "$$\n",
    "\n",
    "We start by computing $1.$ where $L$ is the lower triangular matrix found in (a), and so \n",
    "\n",
    "\\begin{align*}\n",
    "Lc &= Pb \\\\\n",
    "\\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "\\frac12 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{bmatrix}\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "6.2 \\\\\n",
    "2.9\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "And so \n",
    "\n",
    "\\begin{align*}\n",
    "\\Rightarrow \\underline{c_1 = 6.2} \\\\\n",
    "\\frac12 c_1 + c_2 &= 2.9 \\Rightarrow \\underline{c_2 = 2.9 - 3.1 = -0.2}\n",
    "\\end{align*}\n",
    "\n",
    "Futher we can use\n",
    "$$\n",
    "c = \\begin{bmatrix} 6.2 \\\\ -0.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "to compute $2.$ where $U$ is the upper triangular matrix found in (a), and so\n",
    "\n",
    "\\begin{align*}\n",
    "Ux &= c \\\\\n",
    "\\Rightarrow \n",
    "\\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "0 & -\\frac\\epsilon2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "x_1 \\\\ \n",
    "x_2 \n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix} \n",
    "6.2 \\\\ \n",
    "-0.2 \n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "And so we can start from the bottom and compute $x_2$ then compute $x_1$\n",
    "\n",
    "\\begin{align*}\n",
    "-\\frac\\epsilon2 x_2 = -0.2 \\Rightarrow \\underline{x_2 = \\frac{0.4}{\\epsilon}} \\\\\n",
    "\n",
    "2x_1 + (4 + \\epsilon)x_2 &= 6.2 \\\\\n",
    "2x_1 &= 6.2 - (4 + \\epsilon)\\left(\\frac{0.4}{\\epsilon} \\right) \\\\\n",
    "&= 6.2 - \\frac{1.6}{\\epsilon} - 0.4 \\\\\n",
    "&= 5.8 - \\frac{1.6}{\\epsilon} \\\\\n",
    "\n",
    "\\Rightarrow \\underline{x_1 = 2.9 - \\frac{0.8}{\\epsilon}}\n",
    "\\end{align*}\n",
    "\n",
    "So the solution to this system is\n",
    "$$\n",
    "\\boxed{x = \\begin{bmatrix} 2.9 - \\frac{0.8}{\\epsilon} \\\\ \\frac{0.4}{\\epsilon} \\end{bmatrix}}\n",
    "$$\n",
    "\n",
    "If now we have $\\epsilon = 0.01$, then we get the solution\n",
    "$$\n",
    "\\boxed{x = \\begin{bmatrix} -77.1 \\\\ 40 \\end{bmatrix}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)\n",
    "\n",
    "If now the values for $b$, $2.9$ and $6.2$ is some measured values. And it turns out that a more precise measurement is given as\n",
    "$$\n",
    "b = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\n",
    "$$\n",
    "Repeat (c) for new $b$. What is now the solution for $\\epsilon = 0.01$? \n",
    "\n",
    "Here again we can solve the system \n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "by using the same procedure as in (c). And so we start pÃ¥ multiplying the matrix $P$ into the system, and so we get\n",
    "$$\n",
    "PAx = Pb = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 6 \\\\ 3 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "And here again we can write $PA$ as the factorization $LU$ where $L$ is the lower triangular matrix and $U$ is the upper triangular matrix. So we need to solve the system\n",
    "$$\n",
    "1.\\ Lc = Pb \\\\\n",
    "2.\\ Ux = c\n",
    "$$\n",
    "\n",
    "We start by computing the first equation, and so\n",
    "\n",
    "\\begin{align*}\n",
    "Lc &= Pb \\\\\n",
    "\\Rightarrow \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "\\frac12 & 1 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{bmatrix}\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "6 \\\\ \n",
    "3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "which gives \n",
    "\\begin{align*}\n",
    "c_1 &= \\underline{6} \\\\\n",
    "\\frac12 c_1 + c_2 &= 3 \\Rightarrow c_2 = 3 - \\frac12 (6) = \\underline{0}\n",
    "\\end{align*}\n",
    "\n",
    "so the vector $c$ is given by \n",
    "$$\n",
    "c = \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can now continue by computing \n",
    "\n",
    "\\begin{align*}\n",
    "Ux &= c \\\\\n",
    "\\Rightarrow \n",
    "\\begin{bmatrix}\n",
    "2 & 4 + \\epsilon \\\\\n",
    "0 & -\\frac\\epsilon2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ x_2\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "6 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "which gives \n",
    "\\begin{align*}\n",
    "-\\frac\\epsilon2 x_2 = 0 \\Rightarrow \\underline{x_2 = 0}\n",
    "2x_1 + (4 + \\epsilon) x_2 &= 6 \\\\\n",
    "2x_1 + (4 + \\epsilon) (0) &= 6 \\\\\n",
    "2x_1 &= 6 \\Rightarrow \\underline{x_2 = 3} \n",
    "\\end{align*}\n",
    "\n",
    "So the solution to the system is \n",
    "$$\n",
    "\\boxed{x = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}}\n",
    "$$\n",
    "\n",
    "When $\\epsilon = 0.01$, then the solution to the system is unchanged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e)\n",
    "\n",
    "If we denote the solution from (c) \n",
    "$$\n",
    "x_a = \\begin{bmatrix} -77.1 \\\\ 40 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and the solution from (d) is denoted \n",
    "$$\n",
    "x_0 = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$x_a$ is the approximated solution for the system \n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\epsilon = 0.01,\\ A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4.01 \\end{bmatrix},\\ b = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can compute using both infinity-norm and $2$-norm. We start by using the infinity-norm. And so the backward error can be computed by\n",
    "$$\n",
    "||b - Ax_a ||_{\\infty} \\\\\n",
    "$$\n",
    "$$\n",
    "Ax_a = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4.01 \\end{bmatrix} \\begin{bmatrix} -77.1 \\\\ 40 \\end{bmatrix} = \n",
    "\\begin{bmatrix} 2.9 \\\\ 6.2 \\end{bmatrix}\n",
    "$$\n",
    "And so the backwards error using the infinity-norm is given as\n",
    "$$\n",
    "||b - Ax_a||_\\infty = ||\\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 2.9 \\\\ 6.2 \\end{bmatrix}||_\\infty\n",
    "= ||\\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}||_{\\infty} = \\boxed{0.2}\n",
    "$$\n",
    "\n",
    "The forward error using the infinity-norm is given by \n",
    "$$\n",
    "||x_0 - x_a||_\\infty \n",
    "$$\n",
    "And so\n",
    "$$\n",
    "||\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} -77.1 \\\\ 40 \\end{bmatrix}||_\\infty \n",
    "= ||\\begin{bmatrix} 80.1 \\\\ -40 \\end{bmatrix}||_\\infty = \\boxed{80.1}\n",
    "$$\n",
    "\n",
    "To comnpute the corresponding error magnification factor, we must first compute the relative backward error and the relative forward error using the infinity-norm. The backward error is defined as \n",
    "$$\n",
    "\\frac{||b - Ax_a||_\\infty}{||b||_\\infty}\n",
    "$$\n",
    "\n",
    "And so we get \n",
    "$$\n",
    "\\frac{||b - Ax_a||_\\infty}{||b||_\\infty} = \\frac{0.2}{||\\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}||_\\infty} = \\frac{0.2}{6} \\approx \\underline{0.0333}  \n",
    "$$\n",
    "\n",
    "And the relative forward error is given by\n",
    "$$\n",
    "\\frac{||x_0 - x_a||_\\infty}{||x_0||_\\infty}\n",
    "$$\n",
    "\n",
    "And so we get that the relative forward error is \n",
    "$$\n",
    "\\frac{||x_0 - x_a||_\\infty}{||x_0||_\\infty} = \\frac{80.1}{||\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}||_\\infty} = \n",
    "\\frac{80.1}{3} = \\underline{26.7}\n",
    "$$\n",
    "\n",
    "We can now compute the error magnification factor for the system $Ax = b$, which is defined using infinity-norm as \n",
    "$$\n",
    "error\\ mf = \\frac{relative\\ forward\\ error}{relative\\ backward\\ error} = \\frac{\\frac{||b - Ax_a||_\\infty}{||b||_\\infty}}{\\frac{||x_0 - x_a||_\\infty}{||x_0||_\\infty}}\n",
    "$$\n",
    "\n",
    "And so the error magnification factor is \n",
    "$$\n",
    "\\frac{26.7}{0.0333} = \\boxed{801.802}\n",
    "$$\n",
    "\n",
    "In (b) we found that the condition number using the infinity-norm is $3612.01$, and the condtion number tells us the maximal error magnification factor. And here we get a value that is well below the maximum, but in fact still high. And so a small change in value, will mean a large difference in answers.\n",
    "\n",
    "Now we will do the same computations using the $2$-norm. The backward error is defined as \n",
    "$$\n",
    "||b - Ax_a||_2\n",
    "$$\n",
    "$$\n",
    "b - Ax_a = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 2.9 \\\\ 6.2 \\end{bmatrix} = \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The $2$-norm of a given vector $x = \\begin{bmatrix} x_1 & x_2 & \\dots & \\end{bmatrix}^T$ is defined as \n",
    "$$\n",
    "||x||_2 = \\sqrt{x_1^2 + x_2^2 + \\dots}\n",
    "$$\n",
    "This is also called the Euclidean norm. And so the backward error is \n",
    "$$\n",
    "||b - Ax_a||_2 = ||\\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}||_2 = \\sqrt{(0.1)^2 + (-0.2)^2} = \\boxed{0.2236}\n",
    "$$\n",
    "\n",
    "Further we can compute the forward error, which is defined as \n",
    "$$\n",
    "||x_0 - x_a||_2\n",
    "$$\n",
    "and so the forward error is then \n",
    "$$\n",
    "\\Rightarrow ||\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} -77.1 \\\\ 40 \\end{bmatrix}||_2\n",
    "= ||\\begin{bmatrix} 80.1 \\\\ -40 \\end{bmatrix}||_2 = \\sqrt{(80.1)^2 + (-40)^2} = \\boxed{89.523}\n",
    "$$\n",
    "\n",
    "Here again we need to find the  relative backward error and the relative forward error.  \n",
    "And so \n",
    "$$\n",
    "\\frac{||b - Ax_a||_2}{||b||_2}\n",
    "$$\n",
    "$$\n",
    "||b||_2 = ||\\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}||_2 = \\sqrt{3^2 + 6^2} = \\underline{6.708}\n",
    "$$\n",
    "\n",
    "And the relative forward error is given as \n",
    "$$\n",
    "\\frac{||x_0 - x_a||_2}{||x_0||_2}\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow ||x_0||_2 = ||\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}||_2 = \\underline{3}\n",
    "$$\n",
    "And so the error magnification factor is given as \n",
    "$$\n",
    "error\\ mf = \\frac{\\frac{||x_0 - x_a||_2}{||x_0||_2}}{\\frac{||b - Ax_a||_2}{||b||_2}} = \\frac{\\frac{89.523}{3}}{\\frac{0.2236}{6.708}} = \\boxed{895.32}\n",
    "$$\n",
    "\n",
    "Here again the value is well below the condition number, which checks out since the condition number is the maximal error magnification factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f)\n",
    "\n",
    "Using the $LU$-factorization we can transform a system into easier and simpler ways of calculating if we have different $b$ vectors. And some systems of the form $Ax = b$ is very sensitive to initial changes in the output $b$. A small change in $b$ can lead to very different results for the vector $x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Consider a set of data points $\\{(x_1, y_1),... ,(x_N, y_N)\\}$. Suppose that the relationship between the $x$'s and $y$'s can be expressed as $y_i = g_k (x_i)$, where $k$ is a parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Here we are given the following function, which is a modification of the Mean Squared Error:\n",
    "$$\n",
    "F(k) = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - g_k (x_i))^4\n",
    "$$\n",
    "What does this function represent? What is the meaning of its minimum? \n",
    "\n",
    "Since this is a modification of the Mean Squared Error, then it is just the double squared mean error. And so it describes the relationship between between the $x$'s and $y$'s. The minimum describes the the minimum error between the $x$'s and $y$'s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "\n",
    "From now on,m we will assume that $g_k (x) = kx$. Using this, we are asked to compute the derivative of $F(k)$ with respect to $k$. \n",
    "$$\n",
    "f(k) = \\frac{\\partial F}{\\partial k}(k)\n",
    "$$\n",
    "\n",
    "And so we can compute\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial F}{\\partial k}(k) &= \\frac{\\partial}{\\partial k}\\left(\\frac{1}{N}\\sum_{i=1}^{N}(y_i - kx_i)^4 \\right) \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\partial}{\\partial k} (y_i - kx_i)^4 \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^{N} 4(y_i - kx_i)^3 (-x_i) \\\\\n",
    "&= -\\frac{4}{N}\\sum_{i=1}^{N} x_i(y_i - kx_i)^3\n",
    "\\end{align*}\n",
    "And so the derivative of $F(k)$ is given as\n",
    "$$\n",
    "\\boxed{f(k) = \\frac{\\partial F}{\\partial k}(k) = -\\frac{4}{N}\\sum_{i=1}^{N} x_i(y_i - kx_i)^3}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)\n",
    "\n",
    "Let us now condsider $N = 5$ and the data points given below with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining each data point given in the problem\n",
    "x1, y1 = -0.14636714, -0.01101692\n",
    "x2, y2 = 1.03206087, 0.46129383\n",
    "x3, y3 = 1.87784546, 0.96929284\n",
    "x4, y4 = 2.9766235, 1.40264061\n",
    "x5, y5 = 3.79677605, 2.0512337\n",
    "# Creating array to hold all data points \n",
    "data_points = np.array([[x1, y1], [x2, y2], [x3, y3], [x4, y4], [x5, y5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asked to use Newton's method to find an approximation of the minimum $\\bar{k}$ of the function $F(k)$, that is when the derivative $f(k) = 0$. We are asked to have a table with all intermediate results along with the errors $e_i = |k_i - r|$ and both ratios $e_i/e_{i-1}$ and $e_i/e_{i-1}^2$.\n",
    "\n",
    "The Newton's method or also called Newton-Raphson method is a method that usually converges faster than linearly convergent methods. This method finds the root of function on the form $f(x) = 0$, where we start by guessing a initial value for the root and use the tangent line at the point $(x_0, f(x_0))$ where $x_0$ is our initial guessed value. The $x$-value where the tangeng line intersects the $x$-axis, is our new value for the approximated root. Doing this over again will yield a better and more accurate approximation, and so we iterate over several values for $x$.  \n",
    "\n",
    "We can find a formula for Newton-Raphson method by considering the formula for point-slope for a point $(x_0, f(x_0))$, given as\n",
    "$$\n",
    "y - f(x_0) = f'(x_0)(x - x_0)\n",
    "$$\n",
    "Setting $y = 0$ and solve for $x$ gives us the approximation for the root\n",
    "$$\n",
    "x = x_0 - \\frac{f(x_0)}{f'(x_0)}\n",
    "$$\n",
    "\n",
    "And so the iterative Newton-Raphson method has the formula \n",
    "$$\n",
    "x_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)},\\ for\\ x = 0, 1, 2, ...\n",
    "$$\n",
    "\n",
    "As we can see from the fomula for the Newton-Raphson method, we need both the function $f(k)$ and its derivative. The function is found in (b), but we need to compute the derivative of $f(k)$ before we can implement it using python. And so we have to\n",
    "$$\n",
    "f'(k) = \\frac{\\partial^2}{\\partial k^2}F(k)\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\Rightarrow f'(k) &= \\frac{\\partial}{\\partial k} \\left(-\\frac{4}{N}\\sum_{i=1}^{N} x_i(y_i - kx_i)^3\\right) \\\\\n",
    "&= -\\frac{4}{N}\\sum_{i=1}^{N} \\frac{\\partial}{\\partial k}\\left(x_i (y_i - kx_i)^3\\right) \\\\\n",
    "&= -\\frac{12}{N}\\sum_{i=1}^{N} x_i(y_i - kx_i)^2 (-x_i) \\\\\n",
    "&= \\underline{\\frac{12}{N}\\sum_{i=1}^{N} x_i^2(y_i - kx_i)^2}\n",
    "\\end{align*}\n",
    "\n",
    "Now that we have both the function $f(k)$ and it's derivative $f'(k)$, we can implement these. This is done below using the data points and $N = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining arrays for x and y values given in the problem\n",
    "x = data_points[:, 0]\n",
    "y = data_points[:, 1]\n",
    "# Defining function f and derivative of f\n",
    "f = lambda k: -(4 / 5) * np.sum(x * (y - k * x)**3)\n",
    "df = lambda k: (12 / 5) * np.sum(x**2 * (y - k * x)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement a function to compute the iterative Newton-Raphson method, the error and both the ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton_Raphson(f, df, k0:float, tol:float, iter:int):\n",
    "    \"\"\"\n",
    "    Function for compute the Newton-Raphson method \n",
    "\n",
    "    Input:\n",
    "        f: function, function we want to find the root for\n",
    "        df: function, derivate of the function f\n",
    "        k_0: float, initial guess value for root\n",
    "        tol: float, \n",
    "        iter: int, total number of iterations to avoid infinite loop\n",
    "    \n",
    "    Output:\n",
    "        k_array: np.ndarray, array containing all intermediate values for k\n",
    "        f_array: np.ndarray, array containing all intermediate values for f\n",
    "    \"\"\"\n",
    "    k = k0\n",
    "    k_array = np.zeros((iter))\n",
    "    f_array = np.zeros((iter))\n",
    "    # Setting inital guess for root and it's f(k) value\n",
    "    k_array[0] = k\n",
    "    f_array[0] = f(k)\n",
    "    for i in range(1, iter):\n",
    "        if df(k) == 0:\n",
    "            raise ZeroDivisionError(f\"We can not divide by zero, this occurs when k = {k}\")\n",
    "        # Computing the new k value\n",
    "        k = k - f(k)/df(k)\n",
    "        # Adding values to arrays \n",
    "        k_array[i] = k\n",
    "        f_array[i] = f(k)\n",
    "        # Stop the loop if our value is below the tolerance\n",
    "        if abs(f(k)) < tol:\n",
    "            # Slicing away the unused elements of both arrays \n",
    "            k_array = k_array[:i+1]\n",
    "            f_array = f_array[:i+1]\n",
    "            break\n",
    "        \n",
    "    return k_array, f_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined a function for the Newton-Raphson method, we can use this to compute the approximated value for $k$, that is $\\bar{k}$. Further we are also asked to compute the error, $e_i = |k_i - r|$ where r is the root. Here we are asked to estimate without knowing the exact value for the root. And so what we can do is to compute a good approximation for the root, and then compute the error using this approximation for the root.  \n",
    "And so, we use the last value for the approximated value of $k$ from Newton-Raphson method as the value for our root $r$. By using this value, we can now compute the error and the ratios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the method function defined above \n",
    "k_values, _ = Newton_Raphson(f, df, 1, 5E-9, 20)\n",
    "# Defining the value for the root\n",
    "root1 = k_values[-1]\n",
    "\n",
    "def error(k:np.ndarray, r:float):\n",
    "    \"\"\"\n",
    "    Function for computing the errors and the ratios\n",
    "\n",
    "    Input:\n",
    "        k: np.ndarray, array containing all intermediate values for k\n",
    "        r: float, our value for the root\n",
    "    \"\"\"\n",
    "    # Computing error e_i\n",
    "    e_i = np.abs(k - r)\n",
    "    # Creating arrays for holding ratio values\n",
    "    lin_ratio = np.zeros(len(e_i))\n",
    "    quad_ratio = np.zeros(len(e_i))\n",
    "    # Setting the first element of the ratios as NaN (not a number)\n",
    "    lin_ratio[0] = np.NaN\n",
    "    quad_ratio[0] = np.NaN\n",
    "    for i in range(1, len(e_i)):\n",
    "        lin_ratio[i] = e_i[i] / e_i[i-1]\n",
    "        quad_ratio[i] = e_i[i] / (e_i[i-1]**2)\n",
    "    \n",
    "    return e_i, lin_ratio, quad_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the arrays we need, then we can plot a table containing $k_i$, $e_i = |k_i - r|$, $e_i / e_{i-1}$ and $e_i / e_{i-1}$. And so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_410c5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_410c5_level0_col0\" class=\"col_heading level0 col0\" >k_i</th>\n",
       "      <th id=\"T_410c5_level0_col1\" class=\"col_heading level0 col1\" >e_i = |k_i - r|</th>\n",
       "      <th id=\"T_410c5_level0_col2\" class=\"col_heading level0 col2\" >e_i/e_i-1</th>\n",
       "      <th id=\"T_410c5_level0_col3\" class=\"col_heading level0 col3\" >e_i/e_i-1^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_410c5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_410c5_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_410c5_row0_col1\" class=\"data row0 col1\" >0.489079</td>\n",
       "      <td id=\"T_410c5_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_410c5_row0_col3\" class=\"data row0 col3\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_410c5_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_410c5_row1_col0\" class=\"data row1 col0\" >0.838950</td>\n",
       "      <td id=\"T_410c5_row1_col1\" class=\"data row1 col1\" >0.328029</td>\n",
       "      <td id=\"T_410c5_row1_col2\" class=\"data row1 col2\" >0.670708</td>\n",
       "      <td id=\"T_410c5_row1_col3\" class=\"data row1 col3\" >1.371370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_410c5_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_410c5_row2_col0\" class=\"data row2 col0\" >0.730891</td>\n",
       "      <td id=\"T_410c5_row2_col1\" class=\"data row2 col1\" >0.219970</td>\n",
       "      <td id=\"T_410c5_row2_col2\" class=\"data row2 col2\" >0.670580</td>\n",
       "      <td id=\"T_410c5_row2_col3\" class=\"data row2 col3\" >2.044270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_410c5_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_410c5_row3_col0\" class=\"data row3 col0\" >0.657776</td>\n",
       "      <td id=\"T_410c5_row3_col1\" class=\"data row3 col1\" >0.146855</td>\n",
       "      <td id=\"T_410c5_row3_col2\" class=\"data row3 col2\" >0.667614</td>\n",
       "      <td id=\"T_410c5_row3_col3\" class=\"data row3 col3\" >3.035030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_410c5_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_410c5_row4_col0\" class=\"data row4 col0\" >0.607341</td>\n",
       "      <td id=\"T_410c5_row4_col1\" class=\"data row4 col1\" >0.096420</td>\n",
       "      <td id=\"T_410c5_row4_col2\" class=\"data row4 col2\" >0.656564</td>\n",
       "      <td id=\"T_410c5_row4_col3\" class=\"data row4 col3\" >4.470840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_410c5_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_410c5_row5_col0\" class=\"data row5 col0\" >0.571012</td>\n",
       "      <td id=\"T_410c5_row5_col1\" class=\"data row5 col1\" >0.060091</td>\n",
       "      <td id=\"T_410c5_row5_col2\" class=\"data row5 col2\" >0.623223</td>\n",
       "      <td id=\"T_410c5_row5_col3\" class=\"data row5 col3\" >6.463653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_410c5_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_410c5_row6_col0\" class=\"data row6 col0\" >0.542517</td>\n",
       "      <td id=\"T_410c5_row6_col1\" class=\"data row6 col1\" >0.031595</td>\n",
       "      <td id=\"T_410c5_row6_col2\" class=\"data row6 col2\" >0.525790</td>\n",
       "      <td id=\"T_410c5_row6_col3\" class=\"data row6 col3\" >8.749912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_410c5_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_410c5_row7_col0\" class=\"data row7 col0\" >0.518886</td>\n",
       "      <td id=\"T_410c5_row7_col1\" class=\"data row7 col1\" >0.007965</td>\n",
       "      <td id=\"T_410c5_row7_col2\" class=\"data row7 col2\" >0.252084</td>\n",
       "      <td id=\"T_410c5_row7_col3\" class=\"data row7 col3\" >7.978548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_410c5_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_410c5_row8_col0\" class=\"data row8 col0\" >0.510608</td>\n",
       "      <td id=\"T_410c5_row8_col1\" class=\"data row8 col1\" >0.000313</td>\n",
       "      <td id=\"T_410c5_row8_col2\" class=\"data row8 col2\" >0.039333</td>\n",
       "      <td id=\"T_410c5_row8_col3\" class=\"data row8 col3\" >4.938466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_410c5_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_410c5_row9_col0\" class=\"data row9 col0\" >0.510920</td>\n",
       "      <td id=\"T_410c5_row9_col1\" class=\"data row9 col1\" >0.000001</td>\n",
       "      <td id=\"T_410c5_row9_col2\" class=\"data row9 col2\" >0.003071</td>\n",
       "      <td id=\"T_410c5_row9_col3\" class=\"data row9 col3\" >9.802863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_410c5_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_410c5_row10_col0\" class=\"data row10 col0\" >0.510921</td>\n",
       "      <td id=\"T_410c5_row10_col1\" class=\"data row10 col1\" >0.000000</td>\n",
       "      <td id=\"T_410c5_row10_col2\" class=\"data row10 col2\" >0.000000</td>\n",
       "      <td id=\"T_410c5_row10_col3\" class=\"data row10 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x167eef90fa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the error function\n",
    "e, r1, r2 = error(k_values, root1)\n",
    "# Using pandas to create a table of the values\n",
    "dataf = pd.DataFrame(np.array([k_values, e, r1, r2]).T, \n",
    "                    columns = [\"k_i\", \"e_i = |k_i - r|\", \"e_i/e_i-1\", \"e_i/e_i-1^2\"])\n",
    "dataf.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)\n",
    "\n",
    "Can we tell if the method converges linearly or quadratically by only considering the table found in (c)? We are asked to explain why.\n",
    "\n",
    "We can see from the table above, more precise the result from the error $e_i$ that when the convergence start to set in, then the number of correct decimal places in the approximation of $k$, $k_i$ approximatly doubles for each iteration. This indicated that the method converges quadratically.\n",
    "\n",
    "We can also check using theorem $1.11$ from the book, which states\n",
    ">Lef $f$ be a twice continuously differentiable and $f(r) = 0$. If $f'(r) \\neq 0$, then Newton-Raphson method is locally and quadratically convergent to $r$.\n",
    "\n",
    "And so, we can use the derivative of our function $f(k)$, which is defined in (c) to compute the value of the derivative at the approximation of the root $r$. And so, we get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 0.5109213575373539\n",
      "The value of f'(r) = 0.7383572821531348\n"
     ]
    }
   ],
   "source": [
    "# Printing value for f'(k) at k = r\n",
    "print(f\"r = {root1}\")\n",
    "print(f\"The value of f'(r) = {df(root1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the value of the derivative $f'(k) \\neq 0$ at the estimatet value for the root, and thus the method is locally and quadratically convergent to the estimated root for $k$, denoted $\\bar{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e)\n",
    "\n",
    "Here we asked what the multiplicity of the root $\\bar{k}$ of $f(k)$, and if it is possible to use the Modified Newton's Method to find a good approximation of $\\bar{k}$ in less iterations. \n",
    "\n",
    "The multipicity of a root is found by checking the value of the first derivative, second derivative, and so on, at the root. If the value is zero we continue to check until we have a derivative which does not have the value zero. If now the amount of zero derivatives at the root $r$ is $m-1$, then the multiplicity of the root $r$ is $m$. \n",
    "\n",
    "We have computed the derivative of $f(k)$ at the estimated root $\\bar{k}$, which is not equal to zero. And so the multipicity of root $\\bar{k}$ is $1$, and the root is then called **simple**.\n",
    "\n",
    "For us to use the Modified Newton's Method, then we need to have a multiplicity $> 1$. And since we here have a multiplicity of one, then it is not possible to use the Modified Newton's Method to find an approximation of the root $\\bar{k}$ which is found in less iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f)\n",
    "\n",
    "We are here asked to repeat what we did in (c), but now use the Mean Squared Error defined as \n",
    "$$\n",
    "MSE(k) = \\frac{1}{N}\\sum_{i=1}^{N} (y_i - g_k(x_i))^2\n",
    "$$\n",
    "In other word, we are asked to find the minimum $\\tilde{k}$ for the Mean Squared Error function by applying the Newton-Raphson method to the functions derivative $MSE'(k)$. \n",
    "\n",
    "First we have to compute the derivative of the Mean Squared Error function, and so\n",
    "\n",
    "\\begin{align*}\n",
    "MSE'(k) &= \\frac{\\partial}{\\partial k}\\left(\\frac{1}{N} \\sum_{i=1}^{N} (y_i - kx_i)^2\\right) \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\partial}{\\partial k}\\left(y_i - kx_i)^2\\right) \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^{N} 2(y_i - kx_i)(-x_i) \\\\\n",
    "&= \\underline{-\\frac{2}{N} \\sum_{i=1}^{N}y_i x_i - k x_i^2}\n",
    "\\end{align*}\n",
    "\n",
    "And we also need the second derivative of $MSE(k)$, and so\n",
    "\n",
    "\\begin{align*}\n",
    "MSE''(k) &= \\frac{\\partial}{\\partial k}\\left(-\\frac{2}{N} \\sum_{i=1}^{N}y_i x_i - k x_i^2\\right) \\\\\n",
    "&= -\\frac{2}{N} \\sum_{i=1}^{N} \\frac{\\partial}{\\partial k}\\left(y_i x_i - k x_i^2 \\right) \\\\\n",
    "&= -\\frac{2}{N} \\sum_{i=1}^{N} -x_i^2 \\\\\n",
    "&= \\underline{\\frac{2}{N} \\sum_{i=1}^{N} x_i^2}\n",
    "\\end{align*}\n",
    "\n",
    "We can now implement the functions found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the functions found above\n",
    "f2 = lambda k: -(2 / 5) * np.sum(y * x - k * x * x)\n",
    "df2 = lambda k: (2 / 5) * np.sum(x * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute an approximation $\\tilde{k}$ using Newton-Raphson method defined in (c), and compute the error which is also defined in (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5113576499969703"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runing Newton-Raphson method\n",
    "ktild_values, _ = Newton_Raphson(f2, df2, 1, 5E-9, 20)\n",
    "# Defining the best approximation for k, k_tilde\n",
    "k_tilde = ktild_values[-1]\n",
    "# Finding error\n",
    "e2, rt1, rt2 = error(ktild_values, k_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a table, same as we did in (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b4c77\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b4c77_level0_col0\" class=\"col_heading level0 col0\" >k_i</th>\n",
       "      <th id=\"T_b4c77_level0_col1\" class=\"col_heading level0 col1\" >e_i = |k_i - r|</th>\n",
       "      <th id=\"T_b4c77_level0_col2\" class=\"col_heading level0 col2\" >e_i/e_i-1</th>\n",
       "      <th id=\"T_b4c77_level0_col3\" class=\"col_heading level0 col3\" >e_i/e_i-1^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b4c77_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b4c77_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_b4c77_row0_col1\" class=\"data row0 col1\" >0.488642</td>\n",
       "      <td id=\"T_b4c77_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_b4c77_row0_col3\" class=\"data row0 col3\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b4c77_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b4c77_row1_col0\" class=\"data row1 col0\" >0.511358</td>\n",
       "      <td id=\"T_b4c77_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
       "      <td id=\"T_b4c77_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
       "      <td id=\"T_b4c77_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x167eef916f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using pandas to create a table of the values\n",
    "dataf2 = pd.DataFrame(np.array([ktild_values, e2, rt1, rt2]).T, \n",
    "                    columns = [\"k_i\", \"e_i = |k_i - r|\", \"e_i/e_i-1\", \"e_i/e_i-1^2\"])\n",
    "dataf2.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is very clear that we have convergence after only one iteration. The reason for this is that Newton-Raphson method uses a linear tangent in at the point $(x_0, f(x_0))$ and the function $RME'(k)$ is a linear function. And so we will create a parallell line on top of the function which of course will intersect the axis at the precise same point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (g)\n",
    "\n",
    "Here we are asked to plot the data points together with the functions $g_{\\bar{k}} (x_i)$ and $g_{\\tilde{k}} (x_i)$, where $\\bar{k}$ is the estimated root found in (e) and $\\tilde{k}$ is the estimated root in (f).\n",
    "\n",
    "First we define the functions for $g_{\\bar{k}} (x_i)$ and $g_{\\tilde{k}} (x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07484596,  0.52775222,  0.96025064,  1.5221192 ,  1.94151048])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a \n",
    "g_bar = np.array([root1 * i for i in data_points[:, 0]])\n",
    "g_tilde = np.array([k_tilde * i for i in data_points[:, 0]])\n",
    "g_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can now be plotted together with the points, and so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGsCAYAAAAVGEevAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABU4ElEQVR4nO3dd1yVdf/H8dcBBETcuLWc4AJcaSa4ujU1TcWRK1MzR67MMsvKclTqnZbanSsz08qRZppZamp2Z+6BExT3QMjNlHOu3x/85I5wgALXOYf38/Hgkec633Px+XiZvP1e3+u6LIZhGIiIiIg4CBezCxARERHJCIUXERERcSgKLyIiIuJQFF5ERETEoSi8iIiIiENReBERERGHovAiIiIiDkXhRURERByKm9kFZIWkpCSuXbuGh4cHLi7KZyIiIo7AZrORkJBA/vz5cXO7e0RxyvBy7do1Tp48aXYZIiIi8gDKli1L4cKF7/q+U4YXDw8PILn53Llzm1xNalarlbCwMHx9fXF1dTW7nCyhHp1HTuhTPToH9egc4uLiOHnyZMrP8btxyvBy+1RR7ty58fLyMrma1KxWKwBeXl5O+4dPPTqPnNCnenQO6tG53G/JhxaEiIiIiENReBERERGHovAiIiIiDkXhRURERByKwouIiIg4FIUXERERcSgKLyIiIuJQFF5ERETEoSi8iIiIiENxyjvsioiISBawWmHLFrhwAUqUgOBgMOFuvwovIiIicn/Ll8OwYXD27P+2lS4Nn3wCISHZWopOG+UQV65coX79+pz9+x+6DBg+fDjz5s3L5KpERMQhLF8OHTumDi4A584lb1++PFvLUXjJIWbOnMmTTz5J6dKlH+jzAwcOZObMmdy4cSOTKxMREbtmtSbPuBhG2vdub3v55eRx2UThJQeIi4tj2bJldOzY8YH34evrS5kyZfjhhx8ysTIREbF7W7aknXH5O8OAM2eSx2UThRcHs2/fPrp27UpAQABt27Zlx44d+Pn5ERYWljJm0KBBtG3blrZt2wKwefNm3N3dqVGjRqp9rV69moCAAC5dupSy7Y033qBNmzZ3nGFp0qQJP/74Y9Y0JiIi9unChcwdlwkUXhxIWFgYvXr1om7duqxYsYKXXnqJYcOG4e7uTvny5VPGffrpp6xcuZKVK1cCsHPnTqpVq5Zmf08//TRly5Zl1qxZAEybNo0//viDuXPnkjdv3jTjAwIC2L9/P4mJiVnUoYiI2J0SJTJ3XCYw9WqjyMhIJkyYwJ9//omHhwetWrXilVdewcPDI83YQ4cOMWbMGMLCwqhYsSLvvfce1atXz5Q6DANiYzNlV/dltUJcnMsdTx3ez/jx42natCnDhw8HoEKFCqxYsYLIyEjc3NxYv34927ZtY/To0ak+d/78eYoWLZpmfxaLheHDhzN06FB8fHxYuHAhixYtolixYgBs3bqVw4cP06dPHwCKFi3KrVu3iIqKolSpUhlvQEREHE9wcPJVRefO3Xndi8WS/H5wcLaVZFp4MQyDoUOHki9fPhYtWsS1a9d48803cXFx4fXXX081NjY2ln79+tGmTRs+/PBDvvnmG/r378+6devw8vJ6yDogKAj++OOhdpMBrkBNnnjC4Pffk495epw7d45t27axevXqVNvd3d2pXLkyAEePHk359d8lJCTcMRBC8qmgihUr8umnnzJv3jwqVaqU8l79+vWpX79+ymtPT08A4uPj01e0iIg4PlfX5MuhO3ZM/qH19wBz+4fYxx9n6/1eTDttFBERwd69e/nggw+oVKkSderUYejQoWl+OAOsWbMGDw8PRo4cSYUKFRg9ejR58uRh7dq1mVJLegOEmQ4fPkyuXLlShQtI/n308/MD/hdebt68ycCBA1m8eDEABQoU4Pr163fc72+//UZERAQ2m43ChQunem/AgAEcPXo05fW1a9cAKFiwYKb1JSIiDiAkBJYtg3/Oupcunbw9m+/zYtrMS5EiRZg7dy4+Pj6ptt+8eTPN2H379lG7dm0s/58yLBYLtWrVYu/evYQ85G+YxZK8QDr7ThtZ2b9/P48/HoDFkv6U6urqitVqTTWLsnXrVsLDw1NmWyIiIvD09KRv374MGTKEBg0aAFC1atU7XiV08OBBXn75ZSZMmMCKFSv45JNPmDZtWsr7J06cSLWWJiwsjOLFi1OoUKEH6l1ERBxYSAi0bZuz77CbL18+gv92fsxms7Fw4UIef/zxNGOjoqKoWLFiqm2FCxcmPDw8U2qxWCBPnkzZ1X1ZrZA7ty3Dsz3VqlXDzc2NSZMm0atXL8LDw3n//fcBqFy5MvHx8URGRjJixAgmT56caoYmKCiIKVOmcO3aNfLnzw/A2bNn6d+/PwMGDKB169aUKVOGZ599loMHD1KtWjVu3ryJu7s7uXLlStnPrl27UgKRiIjkQK6u0Lix2VXYz+MBJk+ezKFDh1i2bFma9+Li4nB3d0+1zd3d/b5XvVitVqzZeNOc9LhdT0brKly4MOPHj2fKlCl89913NGjQgLZt2/LDDz+QN29eQkNDCQwM5NKlS1gsllT7r1ixIlWqVOHHH3/k2Wef5erVq/Tt25cmTZrwwgsvYLVaqV69OsHBwUyZMoXZs2enLIy+vZ+EhATWr1/P7Nmz71v7g/boSHJCj5Az+lSPzkE9Oof09mYxjAe57iVzTZ48mS+++IKpU6fy1FNPpXm/X79++Pr68uqrr6b6zPHjx5k5c2aa8bGxsRw+fDhLazabzWZj/Pjx+Pr60qVLFzZu3Mj169cJCAhgzpw5vPPOOykLbAH27NnD119/zcSJE3Fxuf9Sp19//ZXr16/Trl07ANatW8fOnTt54403sqolERERAKpUqXLPC3JMn3kZN24c33zzDZMnT75jcAEoVqwY0dHRqbZFR0ff8fLfv/P19X3oq5Eym9VqJTQ0FH9/f1wzcJ5w586dXL58mSpVqnDlyhXmz5/PtWvXGDVqFAUKFGDNmjU0bNiQJk2aYLPZWLx4MVOnTk35fI0aNXB1daVEiRKUSMe1+D/++CMNGzZMubHdsWPH6NixI+XKlcuyHh1JTugRckaf6tE5qEfnEBsbm+qmq3djaniZMWMG3377LVOmTKFFixZ3HRcYGMicOXMwDAOLxYJhGOzevZsBAwbcc/+urq52e4AzWtvly5f56KOPiIyMxMfHh/r167Ns2bKUK4TefvvtlLGdO3emc+fOafbRu3fvdH+/d955J9XrZ599Nt2fvc2ef/8zS07oEXJGn+rROahHx5bevkwLL8ePH+c///kP/fr1o3bt2kRFRaW8V6RIEaKiosibNy+enp60aNGCjz76iAkTJtClSxe+/fZb4uLiaNmypVnlZ7uWLVvmqH5FRETuxrT7vGzYsAGr1cpnn31GUFBQqi9IvkJmzZo1AHh7ezNr1ix27dpFSEgI+/btY/bs2XZ3SkhERESynmkzL/369aNfv353ff/vN0eD5OfqrFixIqvLEhERETunBzOKiIiIQ1F4EREREYei8CIiIiLpYhjw3eIY3h6dhJnP6FV4ERERkfu6fBm6dIGOXfLw2bSrpON2LFlG4UVERETuznaLiB/G0qrRSZYsATc3GPFmYQICzCvJ9DvsioiIiH1KjA7n4ooelM+znUnt1/Ni4iYWLnThsccy+HThTKbwIiIiIqkZBue2fE7BiJd5JE8MV2IKcNg6mF27XPD2Nrs4hRcRERH5GyM+mohvX6SC+/fgDr+HNyYmYAH9XyxjdmkptOYlh7hy5Qr169fn7NmzD/T54cOHM2/evEyuSkRE7EnU8UNcXhhABffvSUzKxdxdk6jw4gaeam8/wQUUXnKMmTNn8uSTT1K6dOkH+vzAgQOZOXMmN27cyOTKRETEHqxcCTWDynMu2ofD56vw3c1tvPDv1yhR0v6igv1VJA9t9OjRvP/++8yYMYMrV64QFxfHsmXL6Nix4wPv09fXlzJlyvDDDz9kYqUiImK22ItHGNA/iXbt4NxFT15fswrjqZ10fakmFnPX5d6VwouD2bdvH127diUgIIC2bduyY8cO/Pz8CPvbBffFixenUqVKnDp1ioIFC7J582bc3d2pUaNGqn2tXr2agIAALl26lLLtjTfeoE2bNnecYWnSpAk//vhjlvUmIiLZyLBx+pepuP0SSJFLH2CxwKuvwvfrHqWqv30/+FjhxYGEhYXRq1cv6taty4oVK3jppZcYNmwY7u7ulC9fPmXckCFD6NSpE5MnTwZg586dVKtWLc3+nn76acqWLcusWbMAmDZtGn/88Qdz584lb968acYHBASwf/9+EhMTs6hDERHJDtab54mY14JHol/B3S2RJyrvYf06g8mTwcPD7OruT1cb3ZYUc/f3LK7g6pm+sbiAW+67j7VacbHFPVCJ48ePp2nTpgwfPhyAChUqsGLFCiIjI3Fzc2P9+vVs27aN0aNHp/rc+fPnKVq0aJr9WSwWhg8fztChQ/Hx8WHhwoUsWrSIYsWKAbB161YOHz5Mnz59AChatCi3bt0iKiqKUqVKPVAPIiJirsidy/Hc/yLlc18mNiE3iw5PpcPr/ShU2E7PEd2BwsttS+5x4XrJVtD4b6dLvisK1tg7jy3aCP616X+vV5aFhOiUl65ATcBaKylD5Z07d45t27axevXqVNvd3d2pXLkyAEePHk359d8lJCTgcZco3aRJEypWrMinn37KvHnzqFSpUsp79evXp379+imvPT2TA1y8mQ+0EBGRB2Ik3uT4kmFUdJkHnrD7VG1Ol1pE34l+dru25W502shBHD58mFy5cqUKFwARERH4+fkB/wsvN2/eZODAgSxevBiAAgUKcP369Tvu97fffiMiIgKbzUbhwoVTvTdgwACOHj2a8vratWsAFCxYMNP6EhGRrHflCowYeIZSSV9js1lYsOsNCnb+g3Y9HS+4gGZe/qfzzbu/Z3FN/brDpTuPA9LkwbYnU720Wq3s37+fjD4SwtXVFavVmmoWZevWrYSHh6fMtkRERODp6Unfvn0ZMmQIDRo0AKBq1ap3vEro4MGDvPzyy0yYMIEVK1bwySefMG3atJT3T5w4kWotTVhYGMWLF6dQoUIZrF5ERExhGGzcZKFnTzh7tgp/hc/myWcepdvEhrg5cAJw4NIzmVue7BlrsWJzyX3nsfdQrVo13NzcmDRpEr169SI8PJz3338fgMqVKxMfH09kZCQjRoxg8uTJqWZogoKCmDJlCteuXSN//vwAnD17lv79+zNgwABat25NmTJlePbZZzl48CDVqlXj5s2buLu7kytXrpT97Nq1KyUQiYiIfUv4K4Jz3/XirY8+5OzZJ6hYEQb9+znq1jW7soen00YOomjRorz//vts2LCBZ555huXLl9OuXTseffRRChQoQFhYGDVq1MBms+HqmnqmyM/Pj6pVq/LTTz8BcPXqVfr27UvTpk3p168fAIGBgTRs2JApU6YAcOzYsVQBKCEhgfXr19O5c+ds6lhERB6IYXBuy3ySVgVS3nsL03sO5sUXDfbswSmCC2jmxaG0adOGNm3aAGCz2Xjuuedo0aIFkLzepU6dOgQFBfHqq6/y1VdfkSfP/2Z9Bg0axKRJk+jcuTMFChRg7dq1afY/e/bslF+HhYXh6+ub8vq7774jICAgzb1iRETEfhjxlzn+bX8qui8Dd9h6LJgb1Rcwe7QDLmy5B828OIgdO3bw888/c+bMGfbv38/LL7/M+fPnUy5jPnr0KJUqVaJatWp07dqVN998M9XnGzduTOfOnYmMjEzX9wsLC0s185IrVy7eeuutzGtIREQy1eVDG/jrqwAqui/jVpIbX+x+n7J9N9I8pKzZpWU6zbw4iOjoaD766CMiIyPx8fGhfv36LF26lAIFCgCkChadOnWiU6dOafbRq1evdH+/fwaVO+1PRETsw+/fbyUo9l+QB8Iu+rLbcxHPT6qDi5NOUSi8OIiWLVvSsmVLs8sQERE7EhMDI0bArFmPs/KVNsS7lKL6c/+mS0AGLixxQAovIiIijsawcXLD53R4pTO7Q/MDFn63fMe48bkc4vb+D8tJJ5RERESck/XmBY5/3oqyl/oxrMEQSpWC9eth0r9zRnABzbyIiIg4DNup34ndO44KXn8Rl+jJrXx12b/PcKjnEmUGhRcRERE7Z9yK4fjil3nMZS7khv1nAoko/jV9PqzqkLf3f1gKLyIiInbs2ulDxK5th693ODabhW/2vkL9ARNoVzGHnCO6A615ERERsVObNkHD5kXh1g3OXi7FtAPf0nHCRMrn4OACCi8iIiJ2J/FGFK+/Dk2bwv6jPgxcvJrzgXsI7lTBoR+omFkUXkREROyFYXDuvwtJWFaRs78vwjDghRdg4Zra1H6ikNnV2Q2FFwfRtGlT/Pz88PPzo3LlytSsWZMuXbqwZcuWDO1n69atHD9+PIuqvLvly5fTtGnTdI01DINFixZlcUUiIvbFSLhC2JfdKHXqOfJ6XKdX469Z/p3B3Lng7W12dfZF4cWBvPnmm/z+++9s3ryZxYsXU6tWLfr3788ff/yR7n306tWL6OjoLKzyzlq1asWyZcvSNXbHjh2MHTs2iysSEbEfl49sImpBIL7u35JkdWX+nnFUG7iS9iE58FKidNCZMweSN29eihQpAkCxYsUYOXIkUVFRfPDBB6xatcrk6u7N09MTT0/PdI01DCOLqxERsRPWRI6teJvy8ZNxyWNwLLIiuzwW0XNiXad9LlFm0G/Ng7Jak5eBf/NN8n+tVlPKePbZZwkLC+PUqVMAHDt2jBdeeIGaNWvi7+9Pt27dUk4T3T5t07NnT6ZPnw7A0qVLadGiBdWrV6devXq89957WO/Sy3PPPceMGTPo2rUrgYGBqfYNcPHiRYYNG8bjjz9Ov379mDBhAomJiUDq00bbtm2jadOmfP311wQHB1OjRg1ee+01EhMTOXv2LD179gTAz8+Pbdu2pTw9u2bNmtSvX59x48Zx69atLPjdFBHJPjEx8O83tlExcRIuLgbf7etLYtM9PPuSgsv96LfnQSxfDmXLQpMm0K1b8n/Llk3ens0qVKgAJIcWm83GgAEDKFWqFCtXruTbb7/FarUyefJkgJTTNtOnT6dPnz5s376d8ePH88orr7B27Vree+89li1bxoYNG+76/WbNmsVTTz3F8uXLKVasGP369SMxMZHExESef/554uLiWLBgAcOGDeO3335j0qRJd9zPpUuX+Pnnn5k7dy7Tp0/nl19+4fvvv6dEiRIpwer333+nZs2ajBs3Di8vL77//ns+/fRTfv75Z5YsWZKZv40iItlq1y6oVQtemxzMO8ve44vjy2k9dg5VA7W4JT0UXjJq+XLo2BHOnk29/dy55O3ZHGDy5s0LQExMDPHx8XTp0oVRo0bxyCOPUK1aNdq3b8+xY8cAKFQoeaV6/vz5yZMnD15eXkyYMIHmzZtTunRpWrRoQdWqVQkPD7/r92vYsCG9evWiQoUKjBs3jsuXL/Pf//6XLVu2EBkZyeTJk/H19aVatWq89dZbfPPNN8TExKTZz61bt3jrrbfw8/MjODiY4OBgQkNDcXV1JX/+/AAUKVIEd3d3zp07R968eSlZsiS1atVi9uzZNGrUKLN/K0VEspw1JpJDc7rQtU0EYWFQqhQ0GvQOvd9un2OeS5QZtOYlI6xWGDYM7rQmwzDAYoGXX4a2bcHVNVtKunnzJgDe3t54eXnRtWtXvv/+ew4cOEBERASHDh3Cx8fnjp+tXr06np6eTJs2jWPHjnH06FFOnTpFUFDQXb9frVq1Un7t7e1NuXLlOH78ODabjbJly5I/f/6U0041a9YkKSmJ06dP33Ffjz76aKp9JSUl3XFc3759efPNN1m3bh0NGzakVatWVK1a9d6/MSIidubSntXk2t2Hqnmi+KxXFDPDNzBrFhTSFdAZppmXjNiyJe2My98ZBpw5kzwumxw9ehSASpUqERMTQ8eOHVm9ejXly5dn6NChjBw58q6f3bJlCyEhIURHRxMcHMy0adNShZM7cfvH3ZGsVisuLi543OGfDLdDzN3W0Li7u6d6fbeFus888wwbN25kxIgRxMTEMHToUKZOnXrPOkVE7EZSLGGLXqLo4TYUzB3FgbP+3PD7mCVLFFwelGZeMuLChcwdlwm+++47qlWrRpkyZdi4cSOXLl1i1apVKSHj999/v2soWLp0KR06dGDMmDEAKbMkjz/++F2/35EjR1J+fePGDU6fPo2fnx9Wq5WTJ09y9erVlFNZe/fuxc3NjUceeYSwsLB092T5x1PGpk6dSsuWLenatStdu3Zl9uzZrFixguHDh6d7nyIiZrhxajfX13bHN2/y353f7h1O3Rffp12l9F19KXem8JIRJUpk7rgMunHjBlFRURiGwZUrV1i2bBlr1qxh3rx5ABQoUIDY2FjWr19P9erV2bp1K4sWLcL7b3c38vLyIjw8nKpVq1KgQAH27NnD0aNHcXFxYdasWURFRaVcIXQnq1atol69evj7+/PJJ59QsmRJ6tWrh8VioUyZMowcOZLhw4dz8OBBFixYQOvWrcmXL1+G+sydOzcABw4coFKlSkRERDB27FjeeecdXF1d2bx5s04biYjd2/fLRqpGNidv3iTOXynBr7Ff0uX9Zrq9fybQaaOMCA6G0qW56/PHLRYoUyZ5XBZ4//33CQoKomHDhvTu3ZsTJ04wf/586tatCySvMRk0aBDvvfcezzzzDMuXL+edd97hr7/+IjIyEki+3HnSpElMnz6dwYMHU7hwYZ599ll69+6Nh4cHXbt25fDhw3etoU2bNnz77beEhIQQExPDnDlzcHNzw9XVlf/85z8AdOnShenTp9O0adMHutmcn58fDRo0oEuXLmzevJl3330XHx8fnnvuOTp37kzRokUZPXr0A/wOiohkvcREGDUK6rV+ggNnqrP2YAjna4TS4zUFl0xjOKGYmBhj586dRkxMTObv/LvvDMNiSf5KXuWS/HV723ff3fPjSUlJxs6dO42kpKTMry2L9ejRw5g2bdp9xzlyj+mVE3o0jJzRp3p0DvbS4+k/fzQeq30r5UfDkP6XjRvXbZmyb3vpMSul9+e3Zl4yKiQEli1Lvr7t70qXTt4eEmJOXSIiYhoj4Rph83tQ5vjTtHp0PIUKwXffwbSZBfHOq1v8ZzZNYD2IkJDky6G3bElenFuiRPKpomy6PFpEROzH5SNbuPXbc/h6n8Jqc6FcOVdCp0PJkmZX5rwUXh6Uqys0bmx2Fdnqq6++MrsEERH7YU0kfMV7lI//EFdvGxGXyrMj10Kem1Rft/fPYgovIiIiGRR36RiXVnShUt5d4AIr9vfGt8snPFsjr9ml5QjKhiIiIhmwaxeEtE+iiPshLt8syPzjS2n57jyqKbhkG828iIiIpIM1MZ7JUzx5+21ISqrMS7kX88JrNenVr7TZpeU4Ci8iIiL3Ebn3J1x29GX1F4tJSgqiQwf4aFYbChc2u7KcSeFFRETkbpLiOLpkJH7MgDzwVvsPuVh5Nc8/f/f7lUrWU3gRERG5gxun93Ltp+745T0EwOK9Q3nsxQ9pUcnkwkThRUREJBXDxvEfp1D68mhK503kwtXibIiZT5f3n9Lt/e2ErjYSERH5f4mJsOCDVVS4/hoebomsO9yWc4H76fGagos90aEQEREBjhyB7t1h9+5ncBnYHWvhRoS81pe8+bS4xd4ovIiISI5mJF5n36L3aPnq21y8XIBChSx4/WuhHlVnxxReREQkx7oc9geJm3pQw/sEk569xILwr5g/P+2zd8W+KLyIiEjOY7tF+IpxlI+bgKu3jVPRj+JRrT8/z0DPJXIACi8iIpKjxF06xsXlPaiUbxu4wKoDPSjfeQada+Q3uzRJJ4UXERHJMcK3bKDk8baUyxfD1Zj8/HBxJp3f6YKnp9mVSUZockxERJye1QoTJ0LwMzW4ejM/fxxvRGiZ/fR8W8HFEWnmRUREnNqFQ/vo+lIAmzdbgMJM2L6FcVMepbCPq9mlyQPSzIuIiDgnazxHvh5Oib01eMT6FXnywOefw6dflldwcXCaeREREadz/XQoV9d0p3K+UACa1T3ImPlQoYK5dUnmUHgRERHnYdg4/uM0Sv81ikfyJRJ5rSgbbs6j68SndXt/J6JDKSIiziEumojPW1Ip7wZwg1+PPk3eZvPoFlTU7Mokk2nNi4iIOLyjR2H6BCsV8vxKbEJuFhz+D4+NWMVjCi5OyS7CS2JiIq1bt2bbtm13HTNw4ED8/PxSfW3cuDEbqxQREXtj2AxmzoQ6dVz4cm0rXl82jd+9d9Nz3EA9UNGJmX7aKCEhgREjRhAeHn7PccePH2fy5MnUr18/ZVv+/LoboohITnU5fBtXfxnA5LHLiIurQN261xkybSCPPKIriZydqeHl2LFjjBgxAsMw7jkuMTGRs2fP4u/vT5EiRbKpOhERsUu2JMK+f5/yMWMpVNDKpG6jOFl6MQ0bhlOqVA2zq5NsYOppo+3bt1OvXj0WL158z3ERERFYLBbKlCmTTZWJiIg9iouK4PjsRvjGj8HN1cqaQ13w6zmHl1829EDFHMTUmZdu3bqla1xERATe3t6MHDmS7du3U7x4cYYMGUKjRo2yuEIREbELhsGpTQsofHIIFfLd4FpsPn648B86vdUdT0+wWq1mVyjZyPQ1L+kRERFBfHw8QUFB9OvXj3Xr1jFw4EAWL16Mv7//XT9ntVrt7g/07Xrsra7MpB6dR07oUz3aP6sV1s78mmd8eoEH/BkRTHyNL+jWuyxgxWp1/B7TIyf1eD8W434LTrKJn58fCxYsoF69emnes9ls3LhxI9UC3QEDBlCkSBHGjRuXZnxsbCyHDx/O0npFRCTrXbyYi3feKUfoPg9+e6chB640o9zT7ShQ0OzKJCtVqVIFLy+vu77vEDMvLi4uaa4sKl++PMeOHbvn53x9fe/ZvBmsViuhoaH4+/vj6uqcK+LVo/PICX2qRztlTWDvslk8P3gQf11xJ08eg4Mlf6PXm65Y7nAFtEP2mEE5ocfY2FjCwsLuO84hwsuoUaOwWCx88MEHKduOHDmCr6/vPT/n6upqtwfYnmvLLOrReeSEPtWj/bhx9iCXV3endr59DHvyMj+eGcfChRYqVnS/72cdpceH4cw9prcvu12bHRUVRXx8PABNmzZl1apVfP/995w6dYoZM2awa9cuevToYXKVIiKSaQyD42umk2tDHR7Nt4+o6z5UeOwxtmyBihXNLk7sid2Gl6CgINasWQNA8+bNGTNmDJ999hmtW7fm119/Ze7cuZQuXdrkKkVEJDPcunGRsNmtqHB1KJ654tkc1oJT1ULpNvIZcuUyuzqxN3Zz2ujo0aP3fN2pUyc6deqUnSWJiEg2OL1zI3n3dsY3bzRxiZ58FzGZtq8O0u395a7sduZFREScm2HArFnQpnMp3F1iCT0byBavnfQYO1jBRe7JbmZeREQk54g+e5EXBhXnhx8AfBm1bgOvT6yJ/yMeZpcmDkAzLyIikn1sVo4ufx/vDWW5EvYb7u7w0UfwyaLHKa3gIumkmRcREckW8dEnObu0J375t0AuePGp5Uzv3ZDAQLMrE0ej8CIiIlnu5KZFFI54iYr5r3MjzpsfLsyg0wc98cz9gDu0WmHLFrhwAUqUgCeeyNR6xb4pvIiISJaxxV/lyMKXqOr1DXjC9hNPEF/zK7q/UP7Bd7p8OQwbBmfPpmxyKV2aAkOHQo0aD1+02D2teRERkSxx5gx8MGQ1Vb2+IcnqytcHxlL+xc00bPWQwaVjx1TBBYBz5yg/ciSsWPFwRYtD0MyLiIhkusWLYcAAuHq1O/mSdvNo8LN0nVDvjs8lSjerNXnG5Q7PE7YYBgbgMnw4tG8PTnr7fEmmmRcREck0N84dYffUtgzoc4WrV6FuXQst3pzCM30eMrhA8hqXf864/I0FsJw9mzxOnJpmXkRE5OEZBsfWfkapyFepVSyOyd1GcrrEHN5+m8y7vf+FC5k7ThyWwouIiDyUWzciOf71C1TO+yO4w5ZjzQh87j36Nszkb1SiROaOE4el8CIiIg/s3PYf8dzXh8p5LxGf6MHyiA9pPWIo+fJnwaqE4GAoXRrOnbvjuhcDoHRpLMHBmf+9xa5ozYuIiGSYYcCmefMpdaw1hfNc4tD56mz22kG3sS9nTXCB5EW4n3yS/Ot/LKAx/v+1bepULdbNARReREQkQ6KioF07CBn6DKejy7DswHDydd7BU539s/6bh4TAsmVQqlTq7aVKETFpUvKVRuL0dNpIRETSx2Zl96qVtOrfnshIC+7uhViZdIBB4/Phkp3/FA4JgbZtU91h1/bEE1wNDc3GIsRMCi8iInJf8X+d5sySntTKv5mWfvPYXrg3X38NgYH5zCnI1RUaN/7fa6vVnDrEFDptJCIi93Ry8zckrgygUv7N3IzPQ9N/ubFzJ3qgophGMy8iInJHtvhrHFk4iKpei8ATdp2qR2zgQp7rU9Hs0iSHU3gREZE0Ig/+gfF7N6rmPYXV5sLSw2/x5NC3KFIss+44J/LgdNpIRERSWbwYXuidSNE8pzkRVY4fE7fw7Pj3FFzEbmjmRUREALh+JY7Bw3Lz1VcAjXlzzRJefKc5z1Q2aVGuyF1o5kVEJKczDI6tnUPi0rJs/SUcFxd46y0Y92VHKii4iB3SzIuISA5262Y0xxb1pUreleANozr8h8rdptKggdmVidydwouISA51budaPPb0pkreiyTccmf58ffp9P5w8uU3uzKRe1N4ERHJYYxbcRz8+nWq55oOeeDwhaqcLv01Xcfqxi3iGLTmRUQkB4mKgnmjZyYHF2D5wSHk7biTp55VcBHHoZkXEZEcYu1a6NsXoqMGU2z4RuIfeYmQcS2y97lEIplA4UVExMnFXz7LpZ8+o/27/+GW1ZWqVXNRuvsP1KhhdmUiD0bhRUTEiZ34bRmFwvvRsvwV3m5fgqgS45g4EXLnNrsykQen8CIi4oRsCTc49NVQqnvNh9yw51Rtgnp0o0lbsysTeXgKLyIiTubSwT9I2vIc1fNFYLW5sOzwKHwat6dxU1+zSxPJFFqmJSLiRHYs/oLCe4IpmS+CU9GPsiZhEx3eG0uBQhazSxPJNAovIiJO4Pp1eP556DosmLjE3Kw53INb/9pHmxeCsSi3iJPRaSMREUdmGOzbuJP2fR/jxAlwcanIzDMHGPZOWXLpIdDipBReREQc1K2bfxG2sB/+3it4NPcGjLJN+OorCAoqa3ZpIllKp41ERBzQuZ3ruPq1P9XyLSfJ5kbPdmHs3QtBQWZXJpL1NPMiIuJAjKR4Dix6A/9cH4M3hF3042TJr+k9oZbZpYlkG4UXEREHceVEKNd+6o5/gVAAVh4aSO2+/6Z5WS+TKxPJXgovIiIO4OefYc2nu/ikSyiXrhdhy615tB/bWs8lkhxJ4UVExI7FxdoY9YYL06YBPE8ZnyieGtiTDo8VM7s0EdMovIiI2KkTW1aQsPM9vvr8V6AQgwdbeGnia3jpLJHkcAovIiJ2xpZwk4NfvYy/1+dQDN57dhIVO35Iy5ZmVyZiHxReRETsSOSh7dza3B3//Mew2SwsPzKSZyeMpWhxsysTsR8KLyIi9sCWxIElH1D51nu45bdy5q8y7PVaQIdxjXV7f5F/0Dp1ERGTXb8Oy8e/T3XbO7i5Wvn5SBcSntxHmxcUXETuRDMvIiIm+uMP6NEDLkcOpdI7Szni8jrt3upOLnelFpG7UXgRETHBrZjLbJg9n6dfHY7NZuHRRwtwtf5eOjV0Nbs0Ebun8CIiks3O7f6VXDt70qLYOXoF5yexzAvMmAH58yu4iKSHwouISDYxkhIIXfQW1V0/wsXb4FhkJZ7tH0DzrmZXJuJYFF5ERLLBlZMHubqmOwEF9gGw6nA/avSZQvNyeUyuTMTxKLyIiGSx0B++ouKVfpQrEE/0jcL8lvA5bd9ti6vOEok8EF0qLSKSReLjYdgwGPT6o3i4JrDl2FOcrxlKyFAFF5GHoZkXEZEscHjXWTo9X5qDBwEaMvXAfxn4dj288ujfjCIPS/8XiYhkIltiDPvmDuCR/X7cunyUokXhxx9hxIf1FVxEMolmXkREMsmlI7tI2NiNwPxhAAzvsp6QUX4ULWpyYSJORuFFRORh2ayELp1E5cR3yJU/iXOXS7HbcwH9P2qq2/uLZAGFFxGRh3Az8iTnlvXEv+AWcIV1RztRtvNM2lQrZHZpIk5L4UVE5AFt3Qp/zv2K4U9u4UacN2v/mkHbN3vi7qHpFpGspPAiIpJBSUkwfnzyl4VR+OS5QOX2I+j0QgWzSxPJERReREQy4OzuzRz/cSrvj1+C1epO9+65eGbcf8if3+zKRHIOhRcRkXQwkhLZv2gM/q4TKV3BYHSHj/Bt9wZd9VwikWyn8CIich9XTh3h8uruBBbcDcCPR16gz4dDKFPO5MJEciiFFxGRuzEMDq+aRdnLr1ChYBx/3SzEb/FzeGZMiG7vL2IihRcRkTuIj4ctM96iWcn3wR3+e7wZ+ZrPp329kmaXJpLj6V7VIiL/EBoKjz0G/Se9QNR1H5ZFTKXmK2vxV3ARsQuaeRER+X+2xFhWz/2ZTsPbk5gIRYuWZ1fpE3Qc4G12aSLyNwovIiLApaN7uLXpOZ4pcISgSuvJXfZJ5s2DokUVXETsjV2cNkpMTKR169Zs27btrmMOHTpEp06dCAwMpEOHDhw4cCAbKxQRp2WzcmPbEgrvfIJHChzh4tXiDBtmYdUq9EBFETtlenhJSEjglVdeITw8/K5jYmNj6devH3Xq1GH58uXUrFmT/v37Exsbm42VioizuXnpDGGznqJxgUm4u91iY3g7bjQM5ZkX9UBFEXtmang5duwYnTt35vTp0/cct2bNGjw8PBg5ciQVKlRg9OjR5MmTh7Vr12ZTpSLibI6u/w7rqgCqFNrEzfg8LDszmwajllOpuo/ZpYnIfZgaXrZv3069evVYvHjxPcft27eP2rVrY/n/fwpZLBZq1arF3r17s6FKEXEmSUnw7rvw/thY8ue+yp4zdfnR9h3tX+2jByqKOAhTF+x269YtXeOioqKoWLFiqm2FCxe+56kmEZF/igiLofvzefjzT4Ae1KnrTrfX22I7fdDs0kQkAzIcXl5//XWefvppGjRogGs23WIyLi4Od3f3VNvc3d1JTEy85+esVitWqzUrS8uw2/XYW12ZST06D2fp07De4sA34/C5sYDwA3vIn78wM2YYdO3a0Wl6vBf16BxyUo/3k+Hw4u3tzejRo7l16xbNmzenVatW1KtXL+WUTlbw8PBIE1QSExPx9PS85+fCwsKyrKaHFRoaanYJWU49Og+779NqxXvPHnJFR3PLx4ebNWty+/79CdFnKXR4LDWK7oZC8GqnBVQPaUaJEon8/cyz3feYCdSjc8gJPd5PhsPL22+/zVtvvcWOHTtYu3Ytr776KgAtW7bk6aefpkaNGpldI8WKFSM6OjrVtujoaIre5zpGX19fvLy8Mr2eh2G1WgkNDcXf3z/bZq6ym3p0Hg7R54oVuAwfjuXs2ZRNRunS2KZM4XCuy1S5+gpeRWO5fLMgm2I/45WZHVM9l8ghenxI6tE55IQeY2Nj0zXx8EBrXiwWC3Xr1qVu3bq88sorzJ07ly+++IKFCxdSsmRJOnfuTK9evfDw8HiQ3acRGBjInDlzMAwDi8WCYRjs3r2bAQMG3PNzrq6udnuA7bm2zKIenYfd9rl8OXTuDIaRanPC1SjCt47Fv84BcIetJ5qS58kvCXm89F13Zbc9ZiL16Bycucf09vVAVxvFxMSwevVqBg8eTFBQED/99BO9e/dm5cqVjB07lrVr1/LSSy89yK5TREVFER8fD0CLFi24fv06EyZM4NixY0yYMIG4uDhatmz5UN9DRByY1QrDhqUJLqFUZ3lIe/zrHCAxKRfLIyYR+PI6Au4RXETEsWR45mXgwIH88ccf5MuXj5YtW7JgwQICAgJS3vf19eX69euMHj36oQoLCgrigw8+ICQkBG9vb2bNmsWYMWNYsmQJfn5+zJ492+5OCYlINtqyBf52qsiGhWkMZRQf4rE0gaLFonBfkkDI/Mcgj+n34xSRTJTh8OLj48OsWbPuuUi3Tp06LF26NEP7PXr06D1fBwQEsGLFiowVKyLO68KFlF9eesSH34IbMnzRVMDCv+LW4z85lGJcSjVORJxDhsPLuHHj7jumSJEiFClS5IEKEhFJlxIlwAL7W1bHr3MYHXMtZ8DZmQRs3s8AZmL5+zgRcSp6qrSIOKSYKuU59WZ1AqomP6R1865ghu+Zgi/HkgdYLFC6NAQHm1iliGQFhRcRcThh65dR9GQ/qla9QmxCbtYubEHrX1fhTlLygNuntD/+GJz0qgyRnEyr2ETEYSQlwW8z3sL3UicKeF1h35k6HIicTUjYjv8FF0iecVm2DEJCzCtWRLKMZl5ExCEcPw7PPQeWv1qycfQk1px4jcZD3qVAoVzwWtfkq48uXEhe4xIcrBkXESem8CIids2wJrH6q910G1KXmzchX74G/Oh6nPZjyvxvkKsrNG5sWo0ikr0UXkTEbl05c5xL3/fgX3n3UjrfborUrMJXX8Gjj5a5/4dFxGlpzYuI2B/D4OCqL8i1rgZ+hf8k4ZYH7756ko0b4dFHzS5ORMymmRcRsSvx1/4ibGE/AgouB0/YfrIhnk0W8Gx9pRYRSabwIiJ248TW9Xjtf56Ague5leTG6jPjaT7sVfJ4a/GtiPyPwouImM5mg+nT4fp//8vb7c4THunHhXKLaD+6ttmliYgdUngREVOdP2ejdx8XfvkFXF1GU+rR3Dw9fDCVSurBqyJyZwovImIOw8b+ZdNJCPuWTb9uwtPTg48+cqP3wJHc5ZmvIiKAwouImCAm+gInvulNQOGfoRyM7vYVnUb1pUoVsysTEUeg8CIi2Srs1+/xOd6X6oX/Ii7Rk5+jPmLU7Bdw9zC7MhFxFAovIpItkuJusn/+cGrlnwt5IPRcTeJqLKJdL023iEjGKLyISJaLiICj81+iZeWvsNksrDo+koaDx1KwsLvZpYmIA9IddkUkyxgGzJ8PgYHw0sz3OHS+Ghssv/LMOx8quIjIA9PMi4hkiatnT7B02lr6TR4IQI0a5fDqsJ9m5fRvJhF5OAovIpK5DIODP37FI5cG82KNGyyvUYHgTs15/XVwdVVwEZGHp/AiIpkm4cYVDi8YQI2CS8ATdp1uwMT/VCKgvtmViYgzUXgRkUxxYttGcu/pSY2CZ7mV5MaaM+/yr6Gvkyev/poRkcylv1VE5KHYbLDt83HUyz0Gl3wGxy9V4tyji2g7+jGzSxMRJ6UT0CLywC5cgFatYPr8Cri4GPwU/iLeHXfTsL2Ci4hkHc28iEjGGQY/rzhN936P8tdf4OnZjaXXK9Hxncf0XCIRyXIKLyKSITHRF4n4pg+BrruxJIZSo0YRFi2CqlU12yIi2UOnjUQk3cI2riJhhT/+hX+igNdVJo3czp9/QtWqZlcmIjmJZl5E5L6S4mPYu2AktfPPgjxw6HwANwMW0btXdbNLE5EcSOFFRO7p5uljXPyzG7ULhgGw+vgrNBj4PgV99BhoETGHwouI3JFhwIIFFlx3rqVRgzDOXy3Jwbxf8vRb/9KiXBExlcKLiKRx+TIMGABLl7qQN/cn5MnnxWN9xtKsUmGzSxMRUXgRkdQO/vg1RzeuYenSr3Bzg+d636DNe9Nwd3c1uzQREUDhRUT+X8KNqxz8chC1Cn1NtVowtN0zdH29A7lyXcTVtbjZ5YmIpFB4ERFObPsNj93PUavQaZKsrvx05m3eXxCCpxfs3Wt2dSIiqSm8iORgRlIiu74cQy2PibjkNzgRXZ4zpRfR5s3HAbBarSZXKCKSlsKLSA514QKEf9GDhmWXAvDzsT7U6PMxDUvnNbkyEZF70x12RXKglSshIABe/2I4kdeKsubGdzR/+3OKKbiIiAPQzItIDhJz+RLzJu1g6MSnAShVqj5/BZ2glb+XyZWJiKSfZl5EcoijG9cQt8yfvpU7Uq30QV59FbZtg6oKLiLiYDTzIuLkkuLj2PPFazyW/1PwhiMXqvP5XKj3lNmViYg8GIUXESd27sAeEjd157FChwFYc3wY9Qd+SGUfT5MrExF5cAovIk7IMGDnwqkEGq/jXugWF68V54D3fFq+9ZSeSyQiDk9rXkSczJUr0KULrFweg7vbLX6LaEfCk6H86zkFFxFxDpp5EXEim9dfp0fvfJw9C+65RhEYXJX2r7fHLZdSi4g4D4UXESeQcPM6B+YPplDMHqIid1CpkicLF7pRt26I2aWJiGQ6hRcRB3di+39x39mD2oVOYi3gwqRXNtLnrZZ4e5tdmYhI1lB4EXFQhvUWO+ePpZbH+7gWsHHqr7KcKrmQoR82MLs0EZEspfAi4oCiIsL5a3UPHvPZDsC64z3x7zWdhmXymVyZiEjWU3gRcTArV4LX9pdpVm07V2IK8KdtFk+N7oyLrh0UkRxCf92JOIiYGOjXD9q1g76z/8PG8LZcqrOflv0VXEQkZ9HMi4gDOLrpF9Yu+pM5c98BoHOvR3li1Pd4eJhcmIiICRReROyYNTGeXZ+/Tt380/BrAn+EN6D/O0/StKnZlYmImEfhRcROnT2wn4SN3alb+AAAP0UM5rOl9SlUxOTCRERMpvAiYmcMm41dX3+Mv/UNPAonEnmtGKHeX9BidEvd3l9EBIUXEbty5QocmduF+qWWggv8fqINZTrN5V++Rc0uTUTEbugaBRE7sXEjBATAx8s7EJuQmx+jZvL4yJU8quAiIpKKZl5ETJZw8wYzJ4czfFwtDAN2ez7LEd+GPN2ghNmliYjYJc28iJjoxI6tXFpQgy7FW+KT9xJ9+8KePVBLwUVE5K408yJiAsOaxI7546nlPh63AlbOXH6ExV+cp0mIThGJiNyPwotINouKOE70qh7ULfInAL9GdKNqz09p8kgBcwsTEXEQCi8i2WjP8vlUujqEKkVuci02H1utn9H8zW66vb+ISAYovIhkg5gYGDEC6hmbqdnoJrvOBOPd7Cta1HrU7NJERByOwotIFtu5w0r3Hq6EhcEiz2kUKF+LVsNewsPT1ezSREQcksKLSBaxJiaw4/PRXD5xmLCw1ZQqZeHLL/Py5JNDzC5NRMShKbyIZIFzBw8Qt6E7j/vshxowZsAmhk5oQqFCZlcmIuL4FF5EMpFhs7Hz6+n4W1/H0yeBqOtF2O/1OWP+00TPJRIRySQKLyKZ5OqFC5xa3JvHiv4MLvDHyVaUCpnHk5WLmV2aiIhTMfUCzYSEBN58803q1KlDUFAQ8+bNu+vYgQMH4ufnl+pr48aN2VityN1t2mhwbH4HAov+TFyiJz9FzaDua6t5VMFFRCTTmTrzMmnSJA4cOMCXX37J+fPnef311ylZsiQtWrRIM/b48eNMnjyZ+vXrp2zLnz9/dpYrkkZiIrz7LkyebOGx8lOZ3W8IlvrzaRlc1ezSRESclmnhJTY2lqVLlzJnzhyqVatGtWrVCA8PZ9GiRWnCS2JiImfPnsXf358iRYqYVLFIalePh/P+1INMWtQTAP/G9agwcBveebW4RUQkK5l22ujIkSMkJSVRs2bNlG21a9dm37592Gy2VGMjIiKwWCyUKVMmu8sUScOwWtn+xQc0jH+O0f/qR4NqoSxfDnPnouAiIpINTAsvUVFRFCxYEHd395RtPj4+JCQkcPXq1VRjIyIi8Pb2ZuTIkQQFBdGxY0c2b96czRWLQNTJExya3oj6Xm+Tyy2JP8+1Y+mPpWnf3uzKRERyDtNOG8XFxaUKLkDK68TExFTbIyIiiI+PJygoiH79+rFu3ToGDhzI4sWL8ff3v+v3sFqtWK3WzC/+Idyux97qykxO2aNhsPf7RfheH0K1oje4HpeX1ZFjCXllILnc3Zyr179xymP5D+rROahH55De3iyGYRhZXMsd/fTTT4wfP57//ve/KduOHz9Oq1at2LZtGwUKFEjZbrPZuHHjRqoFugMGDKBIkSKMGzcuzb5jY2M5fPhwltYvOUdcrIWk3z+k8aNLAdh5qj4Xy42hRCUfkysTEXFOVapUwcvL667vmzbzUqxYMa5cuUJSUhJubsllREVF4enpSb58+VKNdXFxSXNlUfny5Tl27Ng9v4evr+89mzeD1WolNDQUf39/XF2d89k2ztTjrl3Q90UX2vvVJKj0cn65MIZGg0cS4GH5X48AW7ZguXgRo3hxCA4GB+/7Nmc6lnejHp2DenQOsbGxhIWF3XecaeGlSpUquLm5sXfvXurUqQPArl278Pf3x8Ul9VKcUaNGYbFY+OCDD1K2HTlyBF9f33t+D1dXV7s9wPZcW2Zx5B6tiQnM/DiSl0c/QlISLLw5kmcGtqbVc8mnKW9Pbbr+8AOuw4fD2bP/+3Dp0vDJJxASYkbpWcKRj2V6qUfnoB4dW3r7Mm3Bbu7cuWnXrh3vvvsu+/fvZ/369cybN4+ePZMvO42KiiI+Ph6Apk2bsmrVKr7//ntOnTrFjBkz2LVrFz169DCrfHFi5w4f5vjMx2lktMLNEkeHDrB3nyv1W6ReX1Xg119x6dw5dXABOHcOOnaE5cuzsWoRkZzD1DvsvvHGG1SrVo3nn3+e9957jyFDhtC8eXMAgoKCWLNmDQDNmzdnzJgxfPbZZ7Ru3Zpff/2VuXPnUrp0aTPLF2djGOxY9CmFttXC12cvJQpeZPGcIyxdCoUL/2Os1UqZf/8b7rRk7Pa2l18GJ15YJyJiFlPvsJs7d24mTpzIxIkT07x39OjRVK87depEp06dsqs0yWGuRUYS8XUfHiu2Btxh2+nmFGv7Bc9UKXnnD2zZgvulS3ffoWHAmTOwZQs0bpwlNYuI5FR6MKPkeKFrV1PidB9qFosiPtGDjVcn0WzEYNxy3X1i0nLxYvp2fuFCJlUpIiK3KbxIjpWYCO+8Y9DcdQr+1aI4fDGAxDqLaNmw+n0/axQvnr5vUqLEQ1YpIiL/pPAiOdKRI9C9O+zebWFRoS+ZOeI/NHxpDHkLeKZvB8HBJBYtSq6oKCx3WvdisSRfdRQcnLmFi4iIuQt2RbKbYbWy9fOJrJs4nN27oVAh+GROGZ5+84P0BxcAV1fOvPpq8q8t/3ie0e3XH3/sNPd7ERGxJwovkmNEnTrNgWlPUj/3KIY0+5hBXXYQGvrgt2O52rQptiVLoFSp1G+ULg3LljnVfV5EROyJThtJjrB7xddUuPwS/sWucSPem62J05i2sA4uDzsx0r598teWLcmLc0uUcKo77IqI2COFF3FqsVevcmD+IOoW/Rpyw75zj+PZZCHN61TIvG/i6qrLoUVEspHCizitXTsNPDc3pW6JPSRZXVl/8W0aDxqNp5f+2IuIODKteRGnY7XChx/C4/UtjFnyJif/Ks/uwlto8doYBRcRESegv8nFqZw7fJTJY8/zybdNADBKdyTvs60pWzQDVxKJiIhd08yLOAfDYPvXMym4rSajG3WmfMmLzJuXfNFPYQUXERGnopkXcXjXIqM49vUL1C22CtzhYOQTbFhvULaK2ZWJiEhWUHgRhxb6808UO9mb2sUiSbjlzqarH/LkK8Pu+VwiERFxbAov4pASE2zsnDWMJ3xmQF44Glmd+FqLeKpRgNmliYhIFtM/T8XhHDkC9Z9w4VBoHADrTg+jZO8dBCq4iIjkCJp5EYdh2Gx8MfsGg1/JT1wcjD3/Mb7Nu9Js1JNmlyYiItlI4UUcQvTpM5z77nlKReYiPv4n/vUvF+bP96ZUKQUXEZGcRuFF7N7uFYsp/9cAAotdpWIBL+Z/fIgeg6vjopOeIiI5ksKL2K3Ya9cJ/WIw9Yp+BV4Qev4xcjVaRM/HKpldmoiImEjhRezS0d//i9e+56hX9ARWmwsbLr5Bw5fG4OmVy+zSRETEZAovYlesVvjo31bauLxImVInOH25LBfLfUXzHkFmlyYiInZCqwbEbpw+DU8+Ca+PcuW5zxbw68le5Omwl7pPK7iIiMj/aOZFzGcYbPtmHku/jWfz5kHkyQMvvVWHJr2/wGIxuzgREbE3Ci9iqutRf3H0qxepV3wFNTq4czrxX7w/w4+KFc2uTERE7JXCi5hm/y/rKBbxPI8Vv0BiUi42Xx3Poh8qkcvd7MpERMSeKbxItkuMi2f7nDcI8vkY8kH4pSrE1lhE8541zS5NREQcgMKLZKujR5KwrgkiqPguADacGUTdfpPIW9DL5MpERMRR6GojyRaGATNnQs1abszf2JlL14vyu9uPPPn6DAUXERHJEM28SJaLPnOed964xmeLqgCwN34E1hZ9CCrrY3JlIiLiiDTzIllq1/fLcVnrz+DADuT3juWjj2Dtz66UUHAREZEHpJkXyRKJsXFs++RFnij+BeSBC9fL8sfGy1Sto1NEIiLycBReJNOF/bGNEnufp2zxCGw2CxsujiL4pXfx9NI10CIi8vAUXiTTWG9Z2Tp3Ao97j8WtsJWzlx/hfNmvaNajodmliYiIE9GaF8kUp09Ds+YW4s/8hpurlfXHOuLebjd1Wyu4iIhI5tLMizwcw2Dxt0n0H5iLa9dcOBs+n5ljN1OgVWUKlyhgdnUiIuKENPMiD+x61GW2TenMlfVDuHYN6taFHzeWptHzXfRARRERyTKaeZEHErpuAz7HnqdeiXPUKuJGfLnXGPR6BXLlAqvV7OpERMSZKbxIhtyKT+DP2aMJ9vkI8kNElC83Ahbxcs8KZpcmIiI5hMKLpNuJvQe5tbkbwUX2A7DxTH9qv/gR5QvlMbkyERHJSRRe5L4MA+bOTqRV0lOUK3KO6Bs+HCnwOU1ef8bs0kREJAfSgl25p6goaNcO+g1wZ/D86Ww705LEZqEEPavgIiIi5tDMi9zVrpUrmfqxGz9sehp3dwjq0p7HXm6Hi6suJRIREfMovEgacTdi2D13OA2KzeHjLoU5GxvKJ7NLEBgIoOAiIiLmUniRVI7+sQOPXd1pUCwcm83Cvht9+GlDIXJ7m12ZiIhIMoUXAcCWZOX3OR9SP8+75CqcxLmrpTn/yAKe7NHE7NJERERSUXgRzpyK5/Ky5jQssQWA3890pkrPmTxWoqDJlYmIiKSlq41yuMWLIaCGJ38eqsL1uLxsSviSBq99S2EFFxERsVMKLznUtegrDOl7kS5d4OpVWHR4Cn/V20fj3j2xuGhRroiI2C+FlxwodP0mbi4JpG3RHri62nj7bdiwOQ/lqpczuzQREZH70pqXHORWQiJbZ71NUKHJuBQwuGV1Z+uGCzzWqJTZpYmIiKSbwksOcXLfYeI3dqdh0T0AbD77AjX7fkzZQroGWkREHIvCi5MzbAb/nf8ZtV1GkLtoPJdjCnE471wajWxvdmkiIiIPRGtenFhUFHTuGE/hv6aT2z2eneeaEd80lAZdFFxERMRxaebFSf20xqB3HwuRkbk5fWARHw7/jUavDMXFVXlVREQcm8KLk4m7EcvOOa/y66/liIx8japVYfbXtQgMrGV2aSIiIplC4cWJHN26i1w7uhNc/CiPdfTAw68Ho8eXIHdusysTERHJPAovTsCWZOX3uZOp7/U2uXySuHCtJGdLf8n4j0qYXZqIiEimU3hxcOfDT3Pph540LLEZgK1nQ6jUfTaPlSpscmUiIiJZQ+HFgX23OIbgK49Ro8QlbsbnYZdlGg1f7a3b+4uIiFPTpScO6Pp16NkTOnbJw8RVIwm9UI+oOntp1LuPgouIiDg9zbw4mNANWxj1tjdrttbExQXy1BpO5cFDyeWRy+zSREREsoXCi4O4lXCLP2a9S1ChD/kopBIR0buZM8+LoCAXNIEmIiI5icKLAzgZGkbsuu40Kr4TgEu2J/hzq438WpMrIiI5kMKLHTNsBr9/OYdaDCdP8ViuxBTkcN7ZNBzZ0ezSRERETKPwYqeiL8Zw/KvuBJdaCcCuc09Sov18nqhY2uTKREREzKXFEnZo7VoIqJWba5djSbjlzsZrH1HzlV8oqeAiIiKimRd7EnczjrffsvHRJ3kAFyZsmM8jbaNo8nig2aWJiIjYDVNnXhISEnjzzTepU6cOQUFBzJs3765jDx06RKdOnQgMDKRDhw4cOHAgGyvNekf/3Mu5z+vgF/MyAIMHw9rNJams4CIiIpKKqeFl0qRJHDhwgC+//JIxY8YwY8YM1q5dm2ZcbGws/fr1o06dOixfvpyaNWvSv39/YmNjTag6c9msNjZ99m/KhdWlYpFDPFN7NetWRzN9OnqgooiIyB2YFl5iY2NZunQpo0ePplq1ajRr1oy+ffuyaNGiNGPXrFmDh4cHI0eOpEKFCowePZo8efLcMeg4kgvHz7J3ajMa538Nd7dbbDv3DK5t9vOvp33MLk1ERMRumRZejhw5QlJSEjVr1kzZVrt2bfbt24fNZks1dt++fdSuXRuLJfnW9xaLhVq1arF3797sK9hqhU2b4Jtvkv9rtT7U7v5cugzPXwOoVfJXYhK8+C1+NnVHfI9PqSKZUq6IiIizMi28REVFUbBgQdzd3VO2+fj4kJCQwNWrV9OMLVq0aKpthQsX5uLFi9lRKixfDmXLQpMm0K1b8n/Llk3enkHXr8PE8YXwvTqYgnmucOhiHS7V3kPDPi/quUQiIiLpYNrVRnFxcamCC5DyOjExMV1j/znun6xWK9aHnCFhxQpcOncGw+Dv0cI4dw46dsS2ZAm0b5+uXf33v/D88y6cPFmOW2fnMLT7DuoPeJtcHrkevk47crsXZ+rpn3JCj5Az+lSPzkE9Oof09mZaePHw8EgTPm6/9vT0TNfYf477p7CwsIcr0mrFf9AgXP4RXAAshoEBWAcPJvSRR8DV9e67uZVE5IbFfPlDA06e7ECJEgk83b8K+WqW4eDhgw9Xox0LDQ01u4QslxN6hJzRp3p0DuoxZzAtvBQrVowrV66QlJSEm1tyGVFRUXh6epIvX740Y6Ojo1Nti46OTnMq6Z98fX3x8vJ68CI3bcL10qW7vm0B3CMjqXHjBjRufMcxJ0OPEbvpedqU28YTfb6kYOVG9Ohzhvr1q+F6j8DjyKxWK6Ghofj7+6tHB5cT+lSPzkE9OofY2Nh0TTyYFl6qVKmCm5sbe/fupU6dOgDs2rULf39/XFxSL8UJDAxkzpw5GIaBxWLBMAx2797NgAED7vk9XF1dH+4A3yO4pPo+ly6lmXkxbAZbvpxHLWMY3iViuBabn6P5PmXmvILs3Xvq4WtzAOrReeSEPtWjc1CPji29fZm2YDd37ty0a9eOd999l/3797N+/XrmzZtHz549geRZmPj4eABatGjB9evXmTBhAseOHWPChAnExcXRsmXLrC2yRIkHGvfX+b/Y9u8ONPToi7dnDHvONyam0X6e6NolC4oUERHJWUy9Sd0bb7xBtWrVeP7553nvvfcYMmQIzZs3ByAoKIg1a9YA4O3tzaxZs9i1axchISHs27eP2bNnP9wpofQIDobSpcFyl6uALBYoUyZ53P/b8NMVElcG8HjpFSQm5WLT1YkEDFtPyUqPZG2tIiIiOYSpzzbKnTs3EydOZOLEiWneO3r0aKrXAQEBrFixIrtKS+bqCp98Ah07JgcVw/jfe7cDzccfg6srcXEwahRMm1aQT3u15amaG7lVZxGNn6iVvTWLiIg4OT1V+n5CQmDZMihVKvX20qWTt4eEcHRbKG2anmHatOS3wvN+RIleu6is4CIiIpLp9FTp9AgJgbZtYcsWuHAheY1LcDA2LGz+7GPqe43izaZPcPDkej7/3IVWrfRQIhERkayi8JJerq6pLoc+f/w855f3okmpdQDkLeDF/l0xFCmZ16QCRUREcgaFlwfwx5LlVL72InVKXSY2ITc7jSkEj+iv2/uLiIhkA4WXDLhxJYY9c4fSsNQ8yAOHI2vh2XQRDQMrm12aiIhIjqEFuxkwdqwLhY0/sdksbI4aRYWBWymn4CIiIpKtNPOSAY2ezM2nX3/NgJpXadSjkdnliIiI5EgKLxnQujW0bh1odhkiIiI5mk4biYiIiENReBERERGHovAiIiIiDkXhRURERByKwouIiIg4FIUXERERcSgKLyIiIuJQFF5ERETEoSi8iIiIiENReBERERGHovAiIiIiDkXhRURERByKwouIiIg4FIUXERERcShuZheQFWw2GwBxcXEmV5KW1WoFIDY2FldXV5OryRrq0XnkhD7Vo3NQj87h9s/t2z/H78ZiGIaRHQVlp7/++ouTJ0+aXYaIiIg8gLJly1K4cOG7vu+U4SUpKYlr167h4eGBi4vOjImIiDgCm81GQkIC+fPnx83t7ieHnDK8iIiIiPPStISIiIg4FIWXLJCQkMCbb75JnTp1CAoKYt68eXcde+jQITp16kRgYCAdOnTgwIED2Vjpg8tIjwMHDsTPzy/V18aNG7Ox2oeTmJhI69at2bZt213HOOpx/Lv09OmoxzIyMpKhQ4dSt25dgoOD+eCDD0hISLjjWEc9lhnp0VGP46lTp3jhhReoWbMmjRs3Zu7cuXcd66jHMSM9OupxzBSGZLqxY8cabdq0MQ4cOGD88ssvRs2aNY2ffvopzbiYmBijQYMGxocffmgcO3bMGDdunPHEE08YMTExJlSdMent0TAMo1mzZsbKlSuNS5cupXwlJCRkc8UPJj4+3hg0aJDh6+tr/Pnnn3cc48jH8bb09GkYjnksbTab0blzZ6Nv375GWFiYsWPHDqNZs2bGhx9+mGasox7LjPRoGI55HK1Wq9G8eXNjxIgRxokTJ4xNmzYZtWrVMn744Yc0Yx31OGakR8NwzOOYWRReMllMTIzh7++f6gfAp59+avTo0SPN2KVLlxpNmzY1bDabYRjJfwE1a9bM+O6777Kt3geRkR4TEhKMKlWqGBEREdlZYqYIDw83nnnmGaNNmzb3/KHuqMfxtvT26ajH8tixY4avr68RFRWVsm3VqlVGUFBQmrGOeiwz0qOjHsfIyEhj2LBhxo0bN1K2DRo0yBgzZkyasY56HDPSo6Mex8yi00aZ7MiRIyQlJVGzZs2UbbVr12bfvn1prlvft28ftWvXxmKxAGCxWKhVqxZ79+7NzpIzLCM9RkREYLFYKFOmTHaX+dC2b99OvXr1WLx48T3HOepxvC29fTrqsSxSpAhz587Fx8cn1fabN2+mGeuoxzIjPTrqcSxatCgff/wx3t7eGIbBrl272LFjB3Xr1k0z1lGPY0Z6dNTjmFmc8iZ1ZoqKiqJgwYK4u7unbPPx8SEhIYGrV69SqFChVGMrVqyY6vOFCxcmPDw82+p9EBnpMSIiAm9vb0aOHMn27dspXrw4Q4YMoVGjRmaUniHdunVL1zhHPY63pbdPRz2W+fLlIzg4OOW1zWZj4cKFPP7442nGOuqxzEiPjnoc/65p06acP3+eJk2a8NRTT6V531GP49/dr0dnOI4PQzMvmSwuLi7VD3Ug5XViYmK6xv5znL3JSI8RERHEx8cTFBTE3LlzadSoEQMHDiQ0NDTb6s1qjnocM8pZjuXkyZM5dOgQw4cPT/OesxzLe/XoDMdx2rRpzJw5k8OHD/PBBx+ked8ZjuP9enSG4/gwNPOSyTw8PNL8D3L7taenZ7rG/nOcvclIjy+99BLPPfcc+fPnB6By5cocPHiQJUuW4O/vnz0FZzFHPY4Z5QzHcvLkyXz55ZdMnToVX1/fNO87w7G8X4/OcBxv15mQkMCrr77KyJEjU4UVZziO9+vRGY7jw9DMSyYrVqwYV65cISkpKWVbVFQUnp6e5MuXL83Y6OjoVNuio6MpWrRottT6oDLSo4uLS8r/XLeVL1+eyMjIbKk1OzjqccwoRz+W48aN44svvmDy5Ml3nIYHxz+W6enRUY9jdHQ069evT7WtYsWK3Lp1K83aHkc9jhnp0VGPY2ZReMlkVapUwc3NLdXCsF27duHv75/mUQWBgYHs2bMH4/9vcmwYBrt37yYwMDA7S86wjPQ4atQo3njjjVTbjhw5Qvny5bOj1GzhqMcxoxz5WM6YMYNvv/2WKVOm8PTTT991nCMfy/T26KjH8ezZswwePDjVD+cDBw5QqFChVOvswHGPY0Z6dNTjmGnMuszJmb399tvG008/bezbt89Yt26dUatWLePnn382DMMwLl26ZMTFxRmGYRg3btwwHn/8cWPcuHFGeHi4MW7cOKNBgwZ2fy8Cw0h/jz///LNRrVo1Y8WKFcbJkyeN6dOnGwEBAcaZM2fMLD/D/nkJsbMcx3+6V5+OeiyPHTtmVKlSxZg6dWqq+2FcunTJMAznOJYZ6dFRj2NSUpIREhJi9OnTxwgPDzc2bdpkPPHEE8b8+fMNw3CO45iRHh31OGYWhZcsEBsba4wcOdKoUaOGERQUZHzxxRcp7/n6+qa618C+ffuMdu3aGf7+/kbHjh2NgwcPmlBxxmWkxyVLlhjNmzc3qlevbrRv397Yvn27CRU/nH/+UHeW4/hP9+vTEY/lrFmzDF9f3zt+GYZzHMuM9uiIx9EwDOPixYvGoEGDjFq1ahkNGjQwPvvss5R7uTjDcTSMjPXoqMcxM+jBjCIiIuJQtOZFREREHIrCi4iIiDgUhRcRERFxKAovIiIi4lAUXkRERMShKLyIiIiIQ1F4EREREYei8CIiIiIOReFFREREHIrCi4iIiDgUhRcRERFxKAovImL3li5dSvXq1Tl16hQAx48fx9/fn/Xr15tcmYiYQQ9mFBG7ZxgGPXv2JE+ePHz22Wf06NGD4sWL89FHH5ldmoiYQOFFRBzCiRMnaNu2Lc2aNWPr1q2sXr2aQoUKmV2WiJhAp41ExCGUK1eOfv36sXr1akaOHKngIpKDKbyIiMM4cuQIrq6ubNu2zexSRMRECi8i4hDWr1/P77//zsyZM1m1ahVbt241uyQRMYnCi4jYvZs3bzJu3DgGDhxIw4YN6dGjB2PGjCEhIcHs0kTEBAovImL3pk6diqenJ7179wZg8ODBxMbG8umnn5pcmYiYQVcbiYiIiEPRzIuIiIg4FIUXERERcSgKLyIiIuJQFF5ERETEoSi8iIiIiENReBERERGHovAiIiIiDkXhRURERByKwouIiIg4FIUXERERcSgKLyIiIuJQFF5ERETEofwfs0rOw9jkabYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data_points[:, 0], g_bar, color=\"blue\", label=r\"$g_{\\bar{k}}(x_i)$\")\n",
    "plt.plot(data_points[:, 0], g_tilde, color=\"orange\", label=r\"$g_{\\tilde{k}}(x_i)$\", linestyle=\"--\")\n",
    "plt.scatter(data_points[:, 0], data_points[:, 1], color=\"red\", label=\"Data points\")\n",
    "plt.xlabel(r\"x\")\n",
    "plt.ylabel(r\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot, we can see that both functions follows the point very good. And so, both are good estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "In this problem, we are going to find an \"approximate solution\" to the initial value problem \n",
    "\n",
    "\\begin{equation}\n",
    "y'(x) = -y(x),\\ \\ \\ y(0) = 1\n",
    "\\end{equation}\n",
    "on the interval $[0, 1]$ by converting it into a system of linear (algebraic) equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "We are aksed to divide the interval $[0,1]$ into three subintervals $[x_0, x_1]$, $[x_1, x_2]$ and $[x_2, x_3]$, each with the length of $h$ which is equal for each interval. What are then the numerical values for $x_0$, $x_1$, $x_2$, $x_3$ and $h$?\n",
    "\n",
    "We can use the numpy module in python, and divide an interval into three subintervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diving interval into 4 points with equal length\n",
    "interval = np.linspace(0, 1, 4)\n",
    "x_0, x_1, x_2, x_3 = interval[0], interval[1], interval[2], interval[3]\n",
    "# Finding the length h\n",
    "h = np.diff(interval)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so the values are: $x_0 = 0.0$, $x_1 = 0.3333333333333333$ or $x_1 = 1/3$, $x_2 = 0.6666666666666666$ or $x_2 = 2/3$, $x_3 = 1.0$  \n",
    "and the length $h = 0.3333333333333333$ or $h = 1/3$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)\n",
    "\n",
    "We are here going to use the approximation for the derivative given by \n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + h) - f(x)}{h}\n",
    "$$\n",
    "We are to apply this approximation to the function $y(x)$ and use $(1)$ to explain that we get the approximated equations given in hte problem.\n",
    "\n",
    "If we use the approximation for the derivative for the function $y(x)$ we get\n",
    "$$\n",
    "y(x) \\approx \\frac{y(x + h) - y(x)}{h}\n",
    "$$\n",
    "We can now use this for each subinterval, and so we start by considering the point $x_0$ which has the approximated derivative given by  \n",
    "\n",
    "\\begin{align*}\n",
    "y'(x_0) \\approx \\frac{y(x_0 + h) - y(x_0)}{h}\n",
    "\\end{align*}\n",
    "But here we can notice that the point $x_0$ has the value $0$, and so we can use the initial value given in the problem $y(x_0) = y(0) = 1$ and that $y'(x) = -y(x)$, and thus we can use \n",
    "$$\n",
    "y'(x_0) = -y(x_0) = -y(0) = -1\n",
    "$$\n",
    "Further we can notice that $h = 1/3$, and so $y(x_0 + h)$ can be rewritten as\n",
    "$$\n",
    "y(x_0 + h) = y(0 + 1/3) = y(1/3) = y(x_1)\n",
    "$$\n",
    "Using all these, we can rewrite the approximated derivative for $x_0$ as \n",
    "$$\n",
    "\\boxed{\\frac{y(x_1) - y(x_0)}{h} \\approx -1}\n",
    "$$\n",
    "\n",
    "We can now consider $x_1$ which has the appriate derivative given as\n",
    "$$\n",
    "y'(x_1) \\approx \\frac{y(x_1 + h) - y(x_1)}{h}\n",
    "$$\n",
    "Here again we can use the same, but $x_1$ has the value $1/3$. And so we can rewrite $y'(x_1)$ as\n",
    "$$\n",
    "y'(x_1) = -y(x_1)\n",
    "$$\n",
    "We can here use the value for $h$ and rewrite $y(x_1 + h)$ by\n",
    "$$\n",
    "y(x_1 + h) = y(1/3 + 1/3) = y(2/3) = y(x_2)\n",
    "$$\n",
    "And so the approximation for the derivative is given as \n",
    "$$\n",
    "\\boxed{\\frac{y(x_2) - y(x_1)}{h} \\approx -y(x_1)}\n",
    "$$\n",
    "\n",
    "And now we can consider $x_2$ which has the approximated derivative given by\n",
    "$$\n",
    "y'(x_2) \\approx \\frac{y(x_2 + h) - y(x_2)}{h}\n",
    "$$\n",
    "And so, using the same procedure as for the two points above. Here $x_2$ has the value $2/3$ and again $y'(x) = -y(x)$, and thus we get \n",
    "$$\n",
    "y'(x_2) = -y(x_2) \n",
    "$$\n",
    "Again we can rewrite $y(x_2 + h)$ by\n",
    "$$\n",
    "y(x_2 + h) = y(2/3 + 1/3) = y(1) = y(x_3)\n",
    "$$\n",
    "And so the approximation for the derivative can be written as\n",
    "$$\n",
    "\\boxed{\\frac{y(x_3) - y(x_2)}{h} \\approx -y(x_2)}\n",
    "$$\n",
    "\n",
    "And so, now we have explained that we get the equations given in the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)\n",
    "\n",
    "We will now pretend that the approximations above are exact. We will further use the notation $y_i = y(x_i)$ for $i = 0, 1, 2, 3$ and show that $y_1$, $y_2$ and $y_3$ must satisfy the system $Ay = b$ with\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 0 & 0 \\\\ h - 1 & 1 & 0 \\\\ 0 & h - 1 & 1 \\end{bmatrix},\\ \\ y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix},\\ \\ b = \\begin{bmatrix} 1 - h \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "We are also asked to explaine why this system has a unique solution $y$.\n",
    "\n",
    "To show this, we can start by considering the equations found in the previous problem, given as\n",
    "\n",
    "\\begin{align*}\n",
    "(1)\\ \\frac{y(x_1) - y(x_0)}{h} &= -1 \\\\\n",
    "(2)\\ \\frac{y(x_2) - y(x_1)}{h} &= -y(x_1) \\\\\n",
    "(3)\\ \\frac{y(x_3) - y(x_2)}{h} &= -y(x_2)\n",
    "\\end{align*}\n",
    "We can further rearange these three so all $y$'s are on the left side, and so we start by considering $(1)$ where we notice $y(x_0) = y_0 = 1$ and so\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{y_1 - y_0}{h} &= -1 \\\\\n",
    "\\frac{y_1 - 1}{h} &= -1 \\\\\n",
    "y_1 - 1 &= -h \\\\\n",
    "\\underline{y_1 + 0\\cdot y_2 + 0\\cdot y_3 = 1 - h}\n",
    "\\end{align*}\n",
    "We can now consider $(2)$ and do the same computations\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{y_2 - y_1}{h} &= -y_1 \\\\\n",
    "y_2 - y_1 &= -h y_1 \\\\\n",
    "h y_1 - y_1 + y_2 &= 0 \\\\\n",
    "\\underline{(h - 1)y_1 + y_2 + 0\\cdot y_3 = 0}\n",
    "\\end{align*}\n",
    "\n",
    "And so, now we will consider the last equation $(3)$ and do the same as for the previous equations \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{y_3 - y_2}{h} &= -y_2 \\\\\n",
    "y_3 - y_2 &= -h y_2 \\\\\n",
    "hy_2 - y_2 + y_3 &= 0 \\\\\n",
    "\\underline{0\\cdot y_1 + (h-1)y_2 + y_3 = 0}\n",
    "\\end{align*}\n",
    "\n",
    "Combining this, we get a system of equations which has matrix form as the system given in the problem. This is easy to see.\n",
    "\n",
    "To explain why this system has a unique solution, then we can use theorem $2.10$ and $2.11$ from the book which states\n",
    "> If $n\\times n$ matrix $A$ is strictly diagonally dominant, then \n",
    "> $(1)$ $A$ is a nonsingular matrix, and \n",
    "> $(2)$ for every vector $(b)$ and every starting guess, the Jacobi or the Gauss-Seidel method applied on the system $Ax = b$ converges to a (unique) solution.\n",
    "\n",
    "That the matrix is strictly diagonally dominant simply means that the dominant value in each row is located on the diagonal, or the abolsute value for the value at the diagonal is greater than the sum of absolute values for all other values in the row.  \n",
    "That is, for each $1 \\leq n \\leq n$, $|a_{ii}| > \\sum_{i \\neq j} |a_{ij}|$. \n",
    "\n",
    "If we use the value found for $h = 1/3$, we get that this is true for this matrix $A$, and thus the system has a unique solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)\n",
    "\n",
    "We are here asked to solve the system given in (c) using the Jacobi or the Gauss-Seidel method with initial guess $(1, 1, 1)$. We are then going to argue, using a known theorem, that the method converges to the unique solution $y$.\n",
    "\n",
    "We can solve the system with either the Jacobi or the Gauss-Seidel method, both which are iterative methods and are in fact closely realated to each other. The only difference between the two methods is that the Gauss-Seidel method uses the most recent updated values for the unknowns at each step, even if the updating happens at the current step. \n",
    "\n",
    "Here we will use the Gauss-Seidel method, and thus it will be the only method considered. \n",
    "\n",
    "The Gauss-Seidel method is as stated earlier, an iterative method for solving a linear system. If we consider a matrix $A$, which can be split into three new matrices $D$, $U$ and $L$. Where $D$ is a diagonal matrix, $U$ is an upper triangular matrix and $L$ is an lower triangular matrix.\n",
    "\n",
    "We can further rewrite the system to\n",
    "$$\n",
    "(D + U + L)x = b\n",
    "$$\n",
    "which can be isolated as \n",
    "$$\n",
    "(L + D)x_{k + 1} = -Ux_{k} + b\n",
    "$$\n",
    "Further rearanging gives the matrix equation\n",
    "$$\n",
    "x_{k + 1} = (L + D)^{-1}b - (L + D)^{-1}Ux_k\n",
    "$$\n",
    "for $k = 0, 1, 2, 3, ...$\n",
    "\n",
    "In our case, $x = y$.\n",
    "\n",
    "And so by implementing this into code, then we can solve the system numerically. We start by defining the mtraix $A$, vector $x_0$ which is our initial guessed values and the vector $b$ all using the value for $h$ found before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix A\n",
    "A = np.array([[1, 0, 0], \n",
    "            [h - 1, 1, 0], \n",
    "            [0, h - 1, 1]])\n",
    "# Vector for initial values\n",
    "initial = np.array([[1],\n",
    "                   [1],\n",
    "                   [1]])\n",
    "# Vector b\n",
    "b = np.array([[1 - h],\n",
    "              [0], \n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further define a function to compute the Gauss-Seidel method, this is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And so the solution to the linear ssytem os equations is y = [[0.66666667 0.44444444 0.2962963 ]]^T\n"
     ]
    }
   ],
   "source": [
    "def Gauss_Seidel(A:np.ndarray, y0:np.ndarray, b:np.ndarray, tol:float, iter:int=1000):\n",
    "    \"\"\"\n",
    "    Function for computing a system of equations using Gauss-Seidel method\n",
    "    Sytem of the form Ay = b.\n",
    "\n",
    "    Input:\n",
    "        A: np.ndarray, array of matrix A\n",
    "        y0: np.ndarray, array containg inital y values\n",
    "        b: np.ndarray, array containing vector b\n",
    "        tol: float, tolerance level of error\n",
    "        iter: int, number of iterations\n",
    "    \n",
    "    Output:\n",
    "        y: np.ndarray, solutions of the system of equations\n",
    "    \"\"\"\n",
    "    y = y0\n",
    "    # Creating the lower and upper triangular matricies and the diagonal matrix\n",
    "    L = np.tril(A, k=-1)\n",
    "    U = np.triu(A, k=1) \n",
    "    D = np.diag(np.diag(A))\n",
    "    for k in range(1, iter):\n",
    "        ycopy = y.copy()\n",
    "        y = np.linalg.inv(D + L)@b - np.linalg.inv(D + L)@(U@ycopy)\n",
    "        # Stopping the loop if the solution has very small changes in value\n",
    "        if (np.linalg.norm(y - ycopy)) < tol:\n",
    "            break\n",
    "    return y\n",
    "\n",
    "solutions = Gauss_Seidel(A, initial, b, 5E-11)\n",
    "print(f\"And so the solution to the linear ssytem os equations is y = {solutions.T}^T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can use the same theorem that is used in (c), and so by that and the fact that matrix $A$ is strictly diagonally dominant leads us to conclude that the method converges to a unique solution $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e)\n",
    "\n",
    "Here we are asked to find a polynomial $p(x)$ of degree $3$ that interpolates the $4$ points $(x_0, y_0)$, $(x_1, y_1)$, $(x_2, y_2)$, $(x_3, y_3)$ where $x$'s is found in (a) and $y$'s is found in (d). We are either going to use Lagrange interpolating formula or Newton's divided difference.\n",
    "\n",
    "Here we will use the Lagrange interpolating formula to find a polynomial $p_3(x)$ of degree $3$ to interpolate using the points\n",
    "\n",
    "\\begin{align*}\n",
    "(x_0, y_0) &= (0, 1) \\\\\n",
    "(x_1, y_1) &= (1/3, 2/3) \\\\\n",
    "(x_2, y_2) &= (2/3, 4/9) \\\\\n",
    "(x_3, y_3) &= (1, 8/27)\n",
    "\\end{align*}\n",
    "Here I have taken the liberty of writing the values as a fraction rather than in decimal form. And so the Lagrange interpolating fomula in our case is given as \n",
    "$$\n",
    "p_3 (x) = y_0 \\frac{(x - x_1)(x - x_2)(x - x_3)}{(x_0 - x_1)(x_0 - x_2)(x_0 - x_3)} + y_1 \\frac{(x - x_0)(x - x_2)(x - x_3)}{(x_1 - x_0)(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_0)(x - x_1)(x - x_3)}{(x_2 - x_0)(x_2 - x_1)(x_2 - x_3)} \\\\ + y_3 \\frac{(x - x_0)(x - x_1)(x - x_2)}{(x_3 - x_0)(x_3 - x_1)(x_3 - x_2)}\n",
    "$$\n",
    "Or alternativly we can express the polynomial as\n",
    "$$\n",
    "p_3 (x) = y_0 L_0 \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "p_3 (x) = 1 \\cdot \\frac{(x - 1/3)(x - 2/3)(x - 1)}{(0 - 1/3)(0 - 2/3)(0 - 1)} + 2/3 \\cdot \\frac{(x - 0)(x - 2/3)(x - 1)}{(1/3 - 0)(1/3 - 2/3)(1/3 - 1)} + 4/9 \\cdot \\frac{(x - 0)(x - 1/3)(x - 1)}{(2/3 - 0)(2/3 - 1/3)(2/3 - 1)} \\\\ + 8/27 \\cdot \\frac{(x - 0)(x - 1/3)(x - 2/3)}{(1 - 0)(1 - 1/3)(1 - 2/3)}\n",
    "$$\n",
    "\n",
    "By solving this we get that the polynomial is given as\n",
    "$$\n",
    "p_3 (x) = \n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
